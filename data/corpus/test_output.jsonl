{"metadata": {"issue_key": "HADOOP-125", "project": "HADOOP", "title": "LocalFileSystem.makeAbsolute bug on Windows", "status": "Closed", "priority": "Minor", "reporter": "Peter Sutter", "created": "2006-04-08T06:43:59+00:00", "updated": "2006-08-03T17:46:35+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop). Problem: if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails. Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three: (1) relative to current directory (subdir/file) (2) relative to current disk (/dir/subdir/file) (3) absolute (c:/dir/subdir/file) So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem: private File makeAbsolute(File f) { if (f.isAbsolute()) { return f; } else { return new File(workingDir, f.toString()); } } Im happy to explain if this explanation is confusing...", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop). Problem: if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails. Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three: (1) relative to current directory (subdir/file) (2) relative to current disk (/dir/subdir/file) (3) absolute (c:/dir/subdir/file) So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem: private File makeAbsolute(File f) { if (f.isAbsolute()) { return f; } else { return new File(workingDir, f.toString()); } } Im happy to explain if this explanation is confusing...\n\nComments:", "output": "LocalFileSystem.makeAbsolute bug on Windows - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: LocalFileSystem.makeAbsolute bug on Windows\n\nDescription: LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop). Problem: if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails. Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three: (1) relative to current directory (subdir/file) (2) relative to current disk (/dir/subdir/file) (3) absolute (c:/dir/subdir/file) So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem: private File makeAbsolute(File f) { if (f.isAbsolute()) { return f; } else { return new File(workingDir, f.toString()); } } Im happy to explain if this explanation is confusing...", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'LocalFileSystem.makeAbsolute bug on Windows' and how was it resolved?", "context": "Title: LocalFileSystem.makeAbsolute bug on Windows\n\nDescription: LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop). Problem: if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails. Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three: (1) relative to current directory (subdir/file) (2) relative to current disk (/dir/subdir/file) (3) absolute (c:/dir/subdir/file) So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem: private File makeAbsolute(File f) { if (f.isAbsolute()) { return f; } else { return new File(workingDir, f.toString()); } } Im happy to explain if this explanation is confusing...", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-129", "project": "HADOOP", "title": "FileSystem should not name files with java.io.File", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "created": "2006-04-11T03:05:09+00:00", "updated": "2006-08-03T17:46:35+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "In Hadoop's FileSystem API, files are currently named using java.io.File. This is confusing, as many methods on that class are inappropriate to call on Hadoop paths. For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file. Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names. For example, new File(\"/foo\") is not absolute on Windows, and prints its path as \"\\\\foo\", which causes confusion. To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI. The advantage of URI is that it can also naturally include the namenode host and port. The disadvantage is that URI does not support tree operations like getParent(). This change will cause a lot of incompatibility. Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "In Hadoop's FileSystem API, files are currently named using java.io.File. This is confusing, as many methods on that class are inappropriate to call on Hadoop paths. For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file. Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names. For example, new File(\"/foo\") is not absolute on Windows, and prints its path as \"\\\\foo\", which causes confusion. To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI. The advantage of URI is that it can also naturally include the namenode host and port. The disadvantage is that URI does not support tree operations like getParent(). This change will cause a lot of incompatibility. Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.\n\nComments:", "output": "FileSystem should not name files with java.io.File - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: FileSystem should not name files with java.io.File\n\nDescription: In Hadoop's FileSystem API, files are currently named using java.io.File. This is confusing, as many methods on that class are inappropriate to call on Hadoop paths. For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file. Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names. For example, new File(\"/foo\") is not absolute on Windows, and prints its path as \"\\\\foo\", which causes confusion. To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI. The advantage of URI is that it can also naturally include the namenode host and port. The disadvantage is that URI does not support tree operations like getParent(). This change will cause a lot of incompatibility. Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'FileSystem should not name files with java.io.File' and how was it resolved?", "context": "Title: FileSystem should not name files with java.io.File\n\nDescription: In Hadoop's FileSystem API, files are currently named using java.io.File. This is confusing, as many methods on that class are inappropriate to call on Hadoop paths. For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file. Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names. For example, new File(\"/foo\") is not absolute on Windows, and prints its path as \"\\\\foo\", which causes confusion. To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI. The advantage of URI is that it can also naturally include the namenode host and port. The disadvantage is that URI does not support tree operations like getParent(). This change will cause a lot of incompatibility. Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-132", "project": "HADOOP", "title": "An API for reporting performance metrics", "status": "Closed", "priority": "Major", "reporter": "David Bowen", "created": "2006-04-14T03:12:36+00:00", "updated": "2006-08-03T17:46:35+00:00", "resolution": "Fixed"}, "description": "I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so. The idea is for the API to be sufficiently abstract that various different implementations can be plugged in. In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia. It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network. I'd be very interested in people's thoughts about what the requirements should be for such an API. - David Bowen", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so. The idea is for the API to be sufficiently abstract that various different implementations can be plugged in. In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia. It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network. I'd be very interested in people's thoughts about what the requirements should be for such an API. - David Bowen\n\nComments:", "output": "An API for reporting performance metrics - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: An API for reporting performance metrics\n\nDescription: I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so. The idea is for the API to be sufficiently abstract that various different implementations can be plugged in. In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia. It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network. I'd be very interested in people's thoughts about what the requirements should be for such an API. - David Bowen", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'An API for reporting performance metrics' and how was it resolved?", "context": "Title: An API for reporting performance metrics\n\nDescription: I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so. The idea is for the API to be sufficiently abstract that various different implementations can be plugged in. In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia. It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network. I'd be very interested in people's thoughts about what the requirements should be for such an API. - David Bowen", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-139", "project": "HADOOP", "title": "Deadlock in LocalFileSystem lock/release", "status": "Closed", "priority": "Major", "reporter": "Igor Bolotin", "created": "2006-04-15T10:39:58+00:00", "updated": "2006-08-03T17:46:36+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 1. First thread locks the file and starts some long-running process. 2. Second thread tries to lock the file and it blocks inside channel lock method. It keeps LocalFileSystem instance \"locked\" as well. 3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is \"locked\" by second thread - both threads are waiting to each other.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 1. First thread locks the file and starts some long-running process. 2. Second thread tries to lock the file and it blocks inside channel lock method. It keeps LocalFileSystem instance \"locked\" as well. 3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is \"locked\" by second thread - both threads are waiting to each other.\n\nComments:", "output": "Deadlock in LocalFileSystem lock/release - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Deadlock in LocalFileSystem lock/release\n\nDescription: LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 1. First thread locks the file and starts some long-running process. 2. Second thread tries to lock the file and it blocks inside channel lock method. It keeps LocalFileSystem instance \"locked\" as well. 3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is \"locked\" by second thread - both threads are waiting to each other.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Deadlock in LocalFileSystem lock/release' and how was it resolved?", "context": "Title: Deadlock in LocalFileSystem lock/release\n\nDescription: LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 1. First thread locks the file and starts some long-running process. 2. Second thread tries to lock the file and it blocks inside channel lock method. It keeps LocalFileSystem instance \"locked\" as well. 3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is \"locked\" by second thread - both threads are waiting to each other.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-149", "project": "HADOOP", "title": "TaskTracker#unJar trashes file modes", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-04-20T00:59:20+00:00", "updated": "2006-08-03T17:46:36+00:00", "resolution": "Won't Fix"}, "description": "Last Changed Rev: 395069 The unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves. I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry. I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it). Perhaps support for jobs as tar(.gz) bundles? (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Last Changed Rev: 395069 The unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves. I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry. I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it). Perhaps support for jobs as tar(.gz) bundles? (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).\n\nComments:", "output": "TaskTracker#unJar trashes file modes - Resolved: Won't Fix"}, "classification": {"task": "classification", "input": "Title: TaskTracker#unJar trashes file modes\n\nDescription: Last Changed Rev: 395069 The unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves. I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry. I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it). Perhaps support for jobs as tar(.gz) bundles? (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).", "output": "Priority: Minor | Status: Closed | Resolution: Won't Fix"}, "qa": {"task": "qa", "question": "What is the issue with 'TaskTracker#unJar trashes file modes' and how was it resolved?", "context": "Title: TaskTracker#unJar trashes file modes\n\nDescription: Last Changed Rev: 395069 The unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves. I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry. I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it). Perhaps support for jobs as tar(.gz) bundles? (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).", "answer": "The issue was resolved as: Won't Fix"}}}
{"metadata": {"issue_key": "HADOOP-151", "project": "HADOOP", "title": "RPC code has socket leak?", "status": "Closed", "priority": "Major", "reporter": "Peter Sutter", "created": "2006-04-20T05:18:25+00:00", "updated": "2006-08-03T17:46:36+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below. The comment above the declaration was a bit of giveaway: //TODO mb@media-style.com: static client or non-static client? private static Client CLIENT; private static class Invoker implements InvocationHandler { private InetSocketAddress address; public Invoker(InetSocketAddress address, Configuration conf) { this.address = address; CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ObjectWritable value = (ObjectWritable) CLIENT.call(new Invocation(method, args), address); return value.get(); } } /** Construct a client-side proxy object that implements the named protocol, * talking to a server at the named address. */ public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) { return Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] { protocol }, new Invoker(addr, conf)); } /** Expert: Make multiple, parallel calls to a set of servers. */ public static Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs, Configuration conf) throws IOException { Invocation[] invocations = new Invocation[params.length]; for (int i = 0; i < params.length; i++) invocations[i] = new Invocation(method, params[i]); CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } Writable[] wrappedValues = CLIENT.call(invocations, addrs); if (method.getReturnType() == Void.TYPE) { return null; } Object[] values = (Object[])Array.newInstance(method.getReturnType(),wrappedValues.length); for (int i = 0; i < values.length; i++) if (wrappedValues[i] != null) values[i] = ((ObjectWritable)wrappedValues[i]).get(); return values; }.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below. The comment above the declaration was a bit of giveaway: //TODO mb@media-style.com: static client or non-static client? private static Client CLIENT; private static class Invoker implements InvocationHandler { private InetSocketAddress address; public Invoker(InetSocketAddress address, Configuration conf) { this.address = address; CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ObjectWritable value = (ObjectWritable) CLIENT.call(new Invocation(method, args), address); return value.get(); } } /** Construct a client-side proxy object that implements the named protocol, * talking to a server at the named address. */ public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) { return Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] { protocol }, new Invoker(addr, conf)); } /** Expert: Make multiple, parallel calls to a set of servers. */ public static Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs, Configuration conf) throws IOException { Invocation[] invocations = new Invocation[params.length]; for (int i = 0; i < params.length; i++) invocations[i] = new Invocation(method, params[i]); CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } Writable[] wrappedValues = CLIENT.call(invocations, addrs); if (method.getReturnType() == Void.TYPE) { return null; } Object[] values = (Object[])Array.newInstance(method.getReturnType(),wrappedValues.length); ", "output": "RPC code has socket leak? - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: RPC code has socket leak?\n\nDescription: In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below. The comment above the declaration was a bit of giveaway: //TODO mb@media-style.com: static client or non-static client? private static Client CLIENT; private static class Invoker implements InvocationHandler { private InetSocketAddress address; public Invoker(InetSocketAddress address, Configuration conf) { this.address = address; CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ObjectWritable value = (ObjectWritable) CLIENT.call(new Invocation(method, args), address); return value.get(); } } /** Construct a client-side proxy object that implements the named protocol, * talking to a server at the named address. */ public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) { return Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] { protocol }, new Invoker(addr, conf)); } /** Expert: Make multiple, parallel calls to a set of servers. */ public static Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs, Configuration conf) throws IOException { Invocation[] invocations = new Invocation[params.length]; for (int i = 0; i < params.length; i++) invocations[i] = new Invocation(method, params[i]); CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } Writable[] wrappedValues = CLIENT.call(invocations, addrs); if (method.getReturnType() == Void.TYPE) { return null; } Object[] values = (Object[])Array.newInstance", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'RPC code has socket leak?' and how was it resolved?", "context": "Title: RPC code has socket leak?\n\nDescription: In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below. The comment above the declaration was a bit of giveaway: //TODO mb@media-style.com: static client or non-static client? private static Client CLIENT; private static class Invoker implements InvocationHandler { private InetSocketAddress address; public Invoker(InetSocketAddress address, Configuration conf) { this.address = address; CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ObjectWritable value = (ObjectWritable) CLIENT.call(new Invocation(method, args), address); return value.get(); } } /** Construct a client-side proxy object that implements the named protocol, * talking to a server at the named address. */ public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) { return Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] { protocol }, new Invoker(addr, conf)); } /** Expert: Make multiple, parallel calls to a set of servers. */ public static Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs, Configuration conf) throws IOException { Invocation[] invocations = new Invocation[params.length]; for (int i = 0; i < params.length; i++) invocations[i] = new Invocation(method, params[i]); CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } Writable[] wrappedValues = CLIENT.call(invocations, addrs); if (method.getReturnType() == Void.TYPE) { return null; } Object[] values = (Object[])Array.newInstance", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-160", "project": "HADOOP", "title": "sleeping with locks held", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-04-23T07:01:20+00:00", "updated": "2006-08-03T17:46:37+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.\n\nComments:", "output": "sleeping with locks held - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: sleeping with locks held\n\nDescription: I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'sleeping with locks held' and how was it resolved?", "context": "Title: sleeping with locks held\n\nDescription: I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-166", "project": "HADOOP", "title": "IPC is unable to invoke methods that use interfaces as parameter", "status": "Closed", "priority": "Minor", "reporter": "Stefan Groschupf", "created": "2006-04-26T06:12:25+00:00", "updated": "2006-08-03T17:46:37+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.\n\nComments:", "output": "IPC is unable to invoke methods that use interfaces as parameter - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: IPC is unable to invoke methods that use interfaces as parameter\n\nDescription: Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'IPC is unable to invoke methods that use interfaces as parameter' and how was it resolved?", "context": "Title: IPC is unable to invoke methods that use interfaces as parameter\n\nDescription: Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-167", "project": "HADOOP", "title": "reducing the number of Configuration & JobConf objects created", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-04-26T10:52:22+00:00", "updated": "2006-08-03T17:46:37+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.\n\nComments:", "output": "reducing the number of Configuration & JobConf objects created - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: reducing the number of Configuration & JobConf objects created\n\nDescription: Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'reducing the number of Configuration & JobConf objects created' and how was it resolved?", "context": "Title: reducing the number of Configuration & JobConf objects created\n\nDescription: Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-17", "project": "HADOOP", "title": "tool to mount ndfs on linux", "status": "Closed", "priority": "Major", "reporter": "John Xing", "created": "2006-02-03T15:59:31+00:00", "updated": "2006-08-03T17:46:25+00:00", "resolution": "Duplicate"}, "description": "tool to mount ndfs on linux. It depends on fuse and fuse-j.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "tool to mount ndfs on linux. It depends on fuse and fuse-j.\n\nComments:", "output": "tool to mount ndfs on linux - Resolved: Duplicate"}, "classification": {"task": "classification", "input": "Title: tool to mount ndfs on linux\n\nDescription: tool to mount ndfs on linux. It depends on fuse and fuse-j.", "output": "Priority: Major | Status: Closed | Resolution: Duplicate"}, "qa": {"task": "qa", "question": "What is the issue with 'tool to mount ndfs on linux' and how was it resolved?", "context": "Title: tool to mount ndfs on linux\n\nDescription: tool to mount ndfs on linux. It depends on fuse and fuse-j.", "answer": "The issue was resolved as: Duplicate"}}}
{"metadata": {"issue_key": "HADOOP-172", "project": "HADOOP", "title": "rpc doesn't handle returning null for a String[]", "status": "Closed", "priority": "Blocker", "reporter": "Owen O'Malley", "created": "2006-04-28T00:15:15+00:00", "updated": "2006-08-03T17:46:38+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "The job tracker gets errors in returning the result from pollForTaskWithClosedJob 060427 100434 Served: pollForTaskWithClosedJob 0 declaredClass = [Ljava.lang.String; instance class = org.apache.hadoop.io.NullWritable at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95) at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The job tracker gets errors in returning the result from pollForTaskWithClosedJob 060427 100434 Served: pollForTaskWithClosedJob 0 declaredClass = [Ljava.lang.String; instance class = org.apache.hadoop.io.NullWritable at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95) at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)\n\nComments:", "output": "rpc doesn't handle returning null for a String[] - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: rpc doesn't handle returning null for a String[]\n\nDescription: The job tracker gets errors in returning the result from pollForTaskWithClosedJob 060427 100434 Served: pollForTaskWithClosedJob 0 declaredClass = [Ljava.lang.String; instance class = org.apache.hadoop.io.NullWritable at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95) at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)", "output": "Priority: Blocker | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'rpc doesn't handle returning null for a String[]' and how was it resolved?", "context": "Title: rpc doesn't handle returning null for a String[]\n\nDescription: The job tracker gets errors in returning the result from pollForTaskWithClosedJob 060427 100434 Served: pollForTaskWithClosedJob 0 declaredClass = [Ljava.lang.String; instance class = org.apache.hadoop.io.NullWritable at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95) at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-184", "project": "HADOOP", "title": "hadoop nightly build and regression test on a cluster", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "created": "2006-05-02T04:38:42+00:00", "updated": "2006-08-03T17:46:39+00:00", "assignee": "Mahadev Konar", "resolution": "Fixed"}, "description": "create a jar file for the tests and have filesystem and mapreduce tests on the cluster", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "create a jar file for the tests and have filesystem and mapreduce tests on the cluster\n\nComments:", "output": "hadoop nightly build and regression test on a cluster - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: hadoop nightly build and regression test on a cluster\n\nDescription: create a jar file for the tests and have filesystem and mapreduce tests on the cluster", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'hadoop nightly build and regression test on a cluster' and how was it resolved?", "context": "Title: hadoop nightly build and regression test on a cluster\n\nDescription: create a jar file for the tests and have filesystem and mapreduce tests on the cluster", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-190", "project": "HADOOP", "title": "Job fails though task succeeded if we fail to exit", "status": "Closed", "priority": "Major", "reporter": "Michael Stack", "created": "2006-05-03T04:28:56+00:00", "updated": "2006-08-03T17:46:39+00:00", "resolution": "Fixed"}, "description": "This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal. My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running, my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running. After ten minutes, TT steps in, kills the child and does cleanup of the successful output. Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails. Below is illustration of the problem using log output: .... 060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a um.jpg 24891 image/jpeg 060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp 24891 image/jpeg 060501 090401 Task task_0001_m_000798_0 is done. ... 060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds Killing. .... 060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0 060501 091410 task_0001_m_000798_0 done; removing files. Then, subsequently.... 060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa -12.out java.io.FileNotFoundException: LocalFS ... and on and on.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal. My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running, my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running. After ten minutes, TT steps in, kills the child and does cleanup of the successful output. Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails. Below is illustration of the problem using log output: .... 060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a um.jpg 24891 image/jpeg 060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp 24891 image/jpeg 060501 090401 Task task_0001_m_000798_0 is done. ... 060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds Killing. .... 060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0 060501 091410 task_0001_m_000798_0 done; removing files. Then, subsequently.... 060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa -12.out java.io.FileNotFoundException: LocalFS ... and on and on.\n\nComments:", "output": "Job fails though task succeeded if we fail to exit - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Job fails though task succeeded if we fail to exit\n\nDescription: This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal. My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running, my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running. After ten minutes, TT steps in, kills the child and does cleanup of the successful output. Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails. Below is illustration of the problem using log output: .... 060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a um.jpg 24891 image/jpeg 060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp 24891 image/jpeg 060501 090401 Task task_0001_m_000798_0 is done. ... 060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds Killing. .... 060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0 060501 091410 task_0001_m_000798_0 done; removing files. Then, subsequently.... 060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa -12.out java.io.FileNotFoundException: LocalFS ... and on and on.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Job fails though task succeeded if we fail to exit' and how was it resolved?", "context": "Title: Job fails though task succeeded if we fail to exit\n\nDescription: This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal. My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running, my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running. After ten minutes, TT steps in, kills the child and does cleanup of the successful output. Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails. Below is illustration of the problem using log output: .... 060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a um.jpg 24891 image/jpeg 060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp 24891 image/jpeg 060501 090401 Task task_0001_m_000798_0 is done. ... 060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds Killing. .... 060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0 060501 091410 task_0001_m_000798_0 done; removing files. Then, subsequently.... 060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa -12.out java.io.FileNotFoundException: LocalFS ... and on and on.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-191", "project": "HADOOP", "title": "add hadoopStreaming to src/contrib", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "created": "2006-05-03T05:10:00+00:00", "updated": "2006-08-03T17:46:40+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks. The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce) TO test the patch: Merge the patch. The only existing file that is modified is trunk/build.xml trunk>ant deploy-contrib trunk>bin/hadoopStreaming : should show usage message trunk>ant test-contrib : should run one test successfully TO add src/contrib/someOtherProject: edit src/contrib/build.xml", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks. The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce) TO test the patch: Merge the patch. The only existing file that is modified is trunk/build.xml trunk>ant deploy-contrib trunk>bin/hadoopStreaming : should show usage message trunk>ant test-contrib : should run one test successfully TO add src/contrib/someOtherProject: edit src/contrib/build.xml\n\nComments:", "output": "add hadoopStreaming to src/contrib - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: add hadoopStreaming to src/contrib\n\nDescription: This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks. The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce) TO test the patch: Merge the patch. The only existing file that is modified is trunk/build.xml trunk>ant deploy-contrib trunk>bin/hadoopStreaming : should show usage message trunk>ant test-contrib : should run one test successfully TO add src/contrib/someOtherProject: edit src/contrib/build.xml", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'add hadoopStreaming to src/contrib' and how was it resolved?", "context": "Title: add hadoopStreaming to src/contrib\n\nDescription: This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks. The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce) TO test the patch: Merge the patch. The only existing file that is modified is trunk/build.xml trunk>ant deploy-contrib trunk>bin/hadoopStreaming : should show usage message trunk>ant test-contrib : should run one test successfully TO add src/contrib/someOtherProject: edit src/contrib/build.xml", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-194", "project": "HADOOP", "title": "Distributed checkup of the file system consistency.", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "created": "2006-05-04T09:11:48+00:00", "updated": "2006-08-03T17:46:40+00:00", "assignee": "Konstantin Shvachko", "resolution": "Fixed"}, "description": "This is a map-reduce based test that checks consistency of the file system by reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions. This could be an alternative to the sequential checkup in dfsck. It would be nice to integrate distributed checkup with dfsck, but I don't yet see how. This test reuses classes defined in HADOOP-193.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "This is a map-reduce based test that checks consistency of the file system by reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions. This could be an alternative to the sequential checkup in dfsck. It would be nice to integrate distributed checkup with dfsck, but I don't yet see how. This test reuses classes defined in HADOOP-193.\n\nComments:", "output": "Distributed checkup of the file system consistency. - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Distributed checkup of the file system consistency.\n\nDescription: This is a map-reduce based test that checks consistency of the file system by reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions. This could be an alternative to the sequential checkup in dfsck. It would be nice to integrate distributed checkup with dfsck, but I don't yet see how. This test reuses classes defined in HADOOP-193.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Distributed checkup of the file system consistency.' and how was it resolved?", "context": "Title: Distributed checkup of the file system consistency.\n\nDescription: This is a map-reduce based test that checks consistency of the file system by reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions. This could be an alternative to the sequential checkup in dfsck. It would be nice to integrate distributed checkup with dfsck, but I don't yet see how. This test reuses classes defined in HADOOP-193.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-198", "project": "HADOOP", "title": "adding owen's examples to exampledriver", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "created": "2006-05-05T07:33:24+00:00", "updated": "2006-08-03T17:46:40+00:00", "assignee": "Mahadev Konar", "resolution": "Fixed"}, "description": "owen's sorter and randomwriter are not added to the examples.jar file", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "owen's sorter and randomwriter are not added to the examples.jar file\n\nComments:", "output": "adding owen's examples to exampledriver - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: adding owen's examples to exampledriver\n\nDescription: owen's sorter and randomwriter are not added to the examples.jar file", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'adding owen's examples to exampledriver' and how was it resolved?", "context": "Title: adding owen's examples to exampledriver\n\nDescription: owen's sorter and randomwriter are not added to the examples.jar file", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-209", "project": "HADOOP", "title": "Add a program to recursively copy directories across file systems", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "created": "2006-05-11T06:05:18+00:00", "updated": "2006-08-03T17:46:41+00:00", "resolution": "Fixed"}, "description": "A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as: hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir \"cp\" command would invoke a map-reduce program to copy files recursively. I willl attach a patch as soon as svn is up and running.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as: hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir \"cp\" command would invoke a map-reduce program to copy files recursively. I willl attach a patch as soon as svn is up and running.\n\nComments:", "output": "Add a program to recursively copy directories across file systems - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Add a program to recursively copy directories across file systems\n\nDescription: A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as: hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir \"cp\" command would invoke a map-reduce program to copy files recursively. I willl attach a patch as soon as svn is up and running.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Add a program to recursively copy directories across file systems' and how was it resolved?", "context": "Title: Add a program to recursively copy directories across file systems\n\nDescription: A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as: hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir \"cp\" command would invoke a map-reduce program to copy files recursively. I willl attach a patch as soon as svn is up and running.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-211", "project": "HADOOP", "title": "logging improvements for Hadoop", "status": "Closed", "priority": "Minor", "reporter": "Sameer Paranjpye", "created": "2006-05-12T06:16:09+00:00", "updated": "2006-08-03T17:46:41+00:00", "assignee": "Sameer Paranjpye", "resolution": "Fixed"}, "description": "Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 broad changes to the way logging is currently done, these being: - The use of a uniform logging format by all Hadoop subsystems - The use of Apache commons logging as a facade above an underlying logging framework - The use of Log4J as the underlying logging framework instead of java.util.logging This is largely polishing work, but it seems like it would make log analysis and debugging easier in the short term. In the long term, it would future proof logging to the extent of allowing the logging framework used to change while requiring minimal code change. The propos changes are motivated by the following requirements which we think Hadoops logging should meet: - Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc. - Log entries should be clearly annotated with a timestamp and a logging level - Log entries should be traceable to the subsystem from which they originated - The logging implementation should allow log entries to be annotated with source code location information like classname, methodname, file and line number, without requiring code changes - It should be possible to change the logging implementation used without having to change thousands of lines of code - The mapping of loggers to destinations (files, directories, servers etc.) should be specified and modifiable via configuration Uniform logging format: All Hadoop logs should have the following structure. \\n \\n [\\n] . . . where the header line specifies the format of each log entry. The header line has the format: '# ...\\n'. The default format of each log entry is: '# Timestamp Level LoggerName Message', where: - Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS - Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.) - LoggerName is the short name of the logging subsystem from which the message originated e.g. fs.FSNamesystem, dfs.Datanode etc. - Message is the log message produced Why Apache commons logging and Log4J? Apache commons logging is a facade meant to be used as a wrapper around an underlying logging implementation. Bridges from Apache commons logging to popular logging implementations (Java logging, Log4J, Avalon etc.) are implemented and available as part of the commons logging distribution. Implementing a bridge to an unsupported implementation is fairly striaghtforward and involves the implementation of subclasses of the commons logging LogFactory and Logger classes. Using Apache commons logging and making all logging calls through it enables us to move to a different logging implementation by simply changing configuration in the best case. Even otherwise, it incurs minimal code churn overhead. Log4J offers a few benefits over java.util.logging that make it a more desirable choice for the logging back end. - Configuration Flexibility: The mapping of loggers to destinations (files, sockets etc.) can be completely specified in configuration. It is possible to do this with Java logging as well, however, configuration is a lot more restrictive. For instance, with Java logging all log files must have names derived from the same pattern. For the namenode, log files could be named with the pattern \"%h/namenode%u.log\" which would put log files in the user.home directory with names like namenode0.log etc. With Log4J it would be possible to configure the namenode to emit log files with different names, say heartbeats.log, namespace.log, clients.log etc. Configuration variables in Log4J can also have the values of system properties embedded in them. - Takes wrappers into account: Log4J takes into account the possibility that an application may be invoking it via a wrapper, such as Apache commons logging. This is important because logging event objects must be able to infer the context of the logging call such as classname, methodname etc. Inferring context is a relatively expensive operation that involves creating an exception and examining the stack trace to find the frame just before the first frame of the logging framework. It is therefore done lazily only when this information actually needs to be logged. Log4J can be instructed to look for the frame corresponding to the wrapper class, Java logging cannot. In the case of Java logging this means that a) the bridge from Apache commons logging is responsible for inferring the calling context and setting it in the logging event and b) this inference has to be done on every logging call regardless of whether or not it is needed. - More handy features: Log4J has some handy features that Java logging doesn't. A couple of examples of these: a) Date based rolling of log files b) Format control through configuration. Log4J has a PatternLayout class that can be configured to generate logs with a user specified pattern. The logging format described above can be described as \"%d{MM/dd/yyyy:HH:mm:SS} %c{2} %p %m\". The format specifiers indicate that each log line should have the date and time followed by the logger name followed by the logging level or priority followed by the application generated message.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 broad changes to the way logging is currently done, these being: - The use of a uniform logging format by all Hadoop subsystems - The use of Apache commons logging as a facade above an underlying logging framework - The use of Log4J as the underlying logging framework instead of java.util.logging This is largely polishing work, but it seems like it would make log analysis and debugging easier in the short term. In the long term, it would future proof logging to the extent of allowing the logging framework used to change while requiring minimal code change. The propos changes are motivated by the following requirements which we think Hadoops logging should meet: - Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc. - Log entries should be clearly annotated with a timestamp and a logging level - Log entries should be traceable to the subsystem from which they originated - The logging implementation should allow log entries to be annotated with source code location information like classname, methodname, file and line number, without requiring code changes - It should be possible to change the logging implementation used without having to change thousands of lines of code - The mapping of loggers to destinations (files, directories, servers etc.) should be specified and modifiable via configuration Uniform logging format: All Hadoop logs should have the following structure. \\n \\n [\\n] . . . where the header line specifies the format of each log entry. The header line has the format: '# ...\\n'. The default format of each log entry is: '# Timestamp Level LoggerName Message', where: - Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS - Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.) - LoggerName is the short name of the logging subsystem from which the message originated e.g. fs.FSNamesystem, dfs.Datanode etc. - Message is the log messa", "output": "logging improvements for Hadoop - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: logging improvements for Hadoop\n\nDescription: Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 broad changes to the way logging is currently done, these being: - The use of a uniform logging format by all Hadoop subsystems - The use of Apache commons logging as a facade above an underlying logging framework - The use of Log4J as the underlying logging framework instead of java.util.logging This is largely polishing work, but it seems like it would make log analysis and debugging easier in the short term. In the long term, it would future proof logging to the extent of allowing the logging framework used to change while requiring minimal code change. The propos changes are motivated by the following requirements which we think Hadoops logging should meet: - Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc. - Log entries should be clearly annotated with a timestamp and a logging level - Log entries should be traceable to the subsystem from which they originated - The logging implementation should allow log entries to be annotated with source code location information like classname, methodname, file and line number, without requiring code changes - It should be possible to change the logging implementation used without having to change thousands of lines of code - The mapping of loggers to destinations (files, directories, servers etc.) should be specified and modifiable via configuration Uniform logging format: All Hadoop logs should have the following structure. \\n \\n [\\n] . . . where the header line specifies the format of each log entry. The header line has the format: '# ...\\n'. The default format of each log entry is: '# Timestamp Level LoggerName Message', where: - Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS - Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.) - LoggerName is the short name of the logging subsystem from which the message originated e.g. fs.FSNam", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'logging improvements for Hadoop' and how was it resolved?", "context": "Title: logging improvements for Hadoop\n\nDescription: Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 broad changes to the way logging is currently done, these being: - The use of a uniform logging format by all Hadoop subsystems - The use of Apache commons logging as a facade above an underlying logging framework - The use of Log4J as the underlying logging framework instead of java.util.logging This is largely polishing work, but it seems like it would make log analysis and debugging easier in the short term. In the long term, it would future proof logging to the extent of allowing the logging framework used to change while requiring minimal code change. The propos changes are motivated by the following requirements which we think Hadoops logging should meet: - Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc. - Log entries should be clearly annotated with a timestamp and a logging level - Log entries should be traceable to the subsystem from which they originated - The logging implementation should allow log entries to be annotated with source code location information like classname, methodname, file and line number, without requiring code changes - It should be possible to change the logging implementation used without having to change thousands of lines of code - The mapping of loggers to destinations (files, directories, servers etc.) should be specified and modifiable via configuration Uniform logging format: All Hadoop logs should have the following structure. \\n \\n [\\n] . . . where the header line specifies the format of each log entry. The header line has the format: '# ...\\n'. The default format of each log entry is: '# Timestamp Level LoggerName Message', where: - Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS - Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.) - LoggerName is the short name of the logging subsystem from which the message originated e.g. fs.FSNam", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-219", "project": "HADOOP", "title": "SequenceFile#handleChecksumException NPE", "status": "Closed", "priority": "Trivial", "reporter": "Michael Stack", "created": "2006-05-16T01:20:43+00:00", "updated": "2006-08-03T17:46:42+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor. The latter is used by ReduceTask Sorter: java.lang.NullPointerException at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400) at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837) at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881) at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766) at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor. The latter is used by ReduceTask Sorter: java.lang.NullPointerException at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400) at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837) at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881) at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766) at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)\n\nComments:", "output": "SequenceFile#handleChecksumException NPE - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: SequenceFile#handleChecksumException NPE\n\nDescription: The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor. The latter is used by ReduceTask Sorter: java.lang.NullPointerException at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400) at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837) at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881) at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766) at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)", "output": "Priority: Trivial | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'SequenceFile#handleChecksumException NPE' and how was it resolved?", "context": "Title: SequenceFile#handleChecksumException NPE\n\nDescription: The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor. The latter is used by ReduceTask Sorter: java.lang.NullPointerException at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400) at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837) at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881) at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766) at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-22", "project": "HADOOP", "title": "remove unused imports", "status": "Closed", "priority": "Trivial", "reporter": "Sami Siren", "created": "2006-02-07T04:33:17+00:00", "updated": "2006-08-03T17:46:26+00:00", "resolution": "Fixed"}, "description": "Following patch will remove unused imports from java source files", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Following patch will remove unused imports from java source files\n\nComments:", "output": "remove unused imports - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: remove unused imports\n\nDescription: Following patch will remove unused imports from java source files", "output": "Priority: Trivial | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'remove unused imports' and how was it resolved?", "context": "Title: remove unused imports\n\nDescription: Following patch will remove unused imports from java source files", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-220", "project": "HADOOP", "title": "Add -dfs and -jt command-line parameters to specify namenode and jobtracker.", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "created": "2006-05-16T05:18:08+00:00", "updated": "2006-08-03T17:46:42+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.\n\nComments:", "output": "Add -dfs and -jt command-line parameters to specify namenode and jobtracker. - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Add -dfs and -jt command-line parameters to specify namenode and jobtracker.\n\nDescription: Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Add -dfs and -jt command-line parameters to specify namenode and jobtracker.' and how was it resolved?", "context": "Title: Add -dfs and -jt command-line parameters to specify namenode and jobtracker.\n\nDescription: Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-228", "project": "HADOOP", "title": "hadoop cp should have a -config option", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "created": "2006-05-18T07:12:00+00:00", "updated": "2006-08-03T17:46:42+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "hadoop cp should have a -config option to enable overriding of default parameters. it would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "hadoop cp should have a -config option to enable overriding of default parameters. it would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job\n\nComments:", "output": "hadoop cp should have a -config option - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: hadoop cp should have a -config option\n\nDescription: hadoop cp should have a -config option to enable overriding of default parameters. it would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'hadoop cp should have a -config option' and how was it resolved?", "context": "Title: hadoop cp should have a -config option\n\nDescription: hadoop cp should have a -config option to enable overriding of default parameters. it would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-229", "project": "HADOOP", "title": "hadoop cp should generate a better number of map tasks", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "created": "2006-05-18T07:22:37+00:00", "updated": "2006-08-03T17:46:42+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times. better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times. better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.\n\nComments:", "output": "hadoop cp should generate a better number of map tasks - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: hadoop cp should generate a better number of map tasks\n\nDescription: hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times. better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'hadoop cp should generate a better number of map tasks' and how was it resolved?", "context": "Title: hadoop cp should generate a better number of map tasks\n\nDescription: hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times. better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-235", "project": "HADOOP", "title": "LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException", "status": "Closed", "priority": "Major", "reporter": "Benjamin Reed", "created": "2006-05-20T01:46:44+00:00", "updated": "2006-08-03T17:46:43+00:00", "resolution": "Fixed"}, "description": "openRaw should throw f.toString() on an error, not toString().", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "openRaw should throw f.toString() on an error, not toString().\n\nComments:", "output": "LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException\n\nDescription: openRaw should throw f.toString() on an error, not toString().", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException' and how was it resolved?", "context": "Title: LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException\n\nDescription: openRaw should throw f.toString() on an error, not toString().", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-241", "project": "HADOOP", "title": "TestCopyFiles fails under cygwin due to incorrect path", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "created": "2006-05-24T01:22:23+00:00", "updated": "2006-08-03T17:46:43+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path. File.getPath() should be used to produce a consistent path.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path. File.getPath() should be used to produce a consistent path.\n\nComments:", "output": "TestCopyFiles fails under cygwin due to incorrect path - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: TestCopyFiles fails under cygwin due to incorrect path\n\nDescription: Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path. File.getPath() should be used to produce a consistent path.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'TestCopyFiles fails under cygwin due to incorrect path' and how was it resolved?", "context": "Title: TestCopyFiles fails under cygwin due to incorrect path\n\nDescription: Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path. File.getPath() should be used to produce a consistent path.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-245", "project": "HADOOP", "title": "record io translator doesn't strip path names", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-05-24T03:41:58+00:00", "updated": "2006-08-03T17:46:43+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "When I run the record translator with a pathname, the path name is not stripped. So for example: % bin/rcc --language c++ foo/bar/bat.jr generates: foo/bar/bat.jr.hh (instead of ./bat.jr.hh) and the first line is #ifndef __FOO/BAR/BAT_JR_HH__ the first was unexpected and the second is clearly wrong.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "When I run the record translator with a pathname, the path name is not stripped. So for example: % bin/rcc --language c++ foo/bar/bat.jr generates: foo/bar/bat.jr.hh (instead of ./bat.jr.hh) and the first line is #ifndef __FOO/BAR/BAT_JR_HH__ the first was unexpected and the second is clearly wrong.\n\nComments:", "output": "record io translator doesn't strip path names - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: record io translator doesn't strip path names\n\nDescription: When I run the record translator with a pathname, the path name is not stripped. So for example: % bin/rcc --language c++ foo/bar/bat.jr generates: foo/bar/bat.jr.hh (instead of ./bat.jr.hh) and the first line is #ifndef __FOO/BAR/BAT_JR_HH__ the first was unexpected and the second is clearly wrong.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'record io translator doesn't strip path names' and how was it resolved?", "context": "Title: record io translator doesn't strip path names\n\nDescription: When I run the record translator with a pathname, the path name is not stripped. So for example: % bin/rcc --language c++ foo/bar/bat.jr generates: foo/bar/bat.jr.hh (instead of ./bat.jr.hh) and the first line is #ifndef __FOO/BAR/BAT_JR_HH__ the first was unexpected and the second is clearly wrong.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-246", "project": "HADOOP", "title": "the record-io generated c++ has wrong comments", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-05-24T03:45:07+00:00", "updated": "2006-08-03T17:46:43+00:00", "assignee": "Milind Barve", "resolution": "Duplicate"}, "description": "The comments on the namespaces on the closing come out backward: } // end namespace org } // end namespace apache } // end namespace hadoop } // end namespace record } // end namespace test", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The comments on the namespaces on the closing come out backward: } // end namespace org } // end namespace apache } // end namespace hadoop } // end namespace record } // end namespace test\n\nComments:", "output": "the record-io generated c++ has wrong comments - Resolved: Duplicate"}, "classification": {"task": "classification", "input": "Title: the record-io generated c++ has wrong comments\n\nDescription: The comments on the namespaces on the closing come out backward: } // end namespace org } // end namespace apache } // end namespace hadoop } // end namespace record } // end namespace test", "output": "Priority: Major | Status: Closed | Resolution: Duplicate"}, "qa": {"task": "qa", "question": "What is the issue with 'the record-io generated c++ has wrong comments' and how was it resolved?", "context": "Title: the record-io generated c++ has wrong comments\n\nDescription: The comments on the namespaces on the closing come out backward: } // end namespace org } // end namespace apache } // end namespace hadoop } // end namespace record } // end namespace test", "answer": "The issue was resolved as: Duplicate"}}}
{"metadata": {"issue_key": "HADOOP-269", "project": "HADOOP", "title": "add FAQ to Wiki", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "created": "2006-06-02T02:04:14+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives.\n\nComments:", "output": "add FAQ to Wiki - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: add FAQ to Wiki\n\nDescription: Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'add FAQ to Wiki' and how was it resolved?", "context": "Title: add FAQ to Wiki\n\nDescription: Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-272", "project": "HADOOP", "title": "bin/hadoop dfs -rm <dir> crashes in log4j code", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-06-03T06:03:04+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages: log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory) at java.io.FileOutputStream.openAppend(Native Method) at java.io.FileOutputStream.(FileOutputStream.java:177) at java.io.FileOutputStream.(FileOutputStream.java:102) at org.apache.log4j.FileAppender.setFile(FileAppender.java:289) at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163) at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215) at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96) at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654) at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612) at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468) at org.apache.log4j.LogManager.(LogManager.java:122) at org.apache.log4j.Logger.getLogger(Logger.java:104) at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229) at org.apache.commons.logging.impl.Log4JLogger.(Log4JLogger.java:65) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:494) at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:529) at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:235) at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:370) at org.apache.hadoop.conf.Configuration.(Configuration.java:54) at org.apache.hadoop.dfs.DFSShell.main(DFSShell.java:307) log4j:ERROR Either File or DatePattern options are not set for appender [DRFA]. Delete failed", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages: log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory) at java.io.FileOutputStream.openAppend(Native Method) at java.io.FileOutputStream.(FileOutputStream.java:177) at java.io.FileOutputStream.(FileOutputStream.java:102) at org.apache.log4j.FileAppender.setFile(FileAppender.java:289) at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163) at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215) at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96) at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654) at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612) at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468) at org.apache.log4j.LogManager.(LogManager.java:122) at org.apache.log4j.Logger.getLogger(Logger.java:104) at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229) at org.apache.commons.logging.impl.Log4JLogger.(Log4JLogger.java:65) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:494) at org.apache.commons.logging.impl.LogFactoryImpl.ne", "output": "bin/hadoop dfs -rm <dir> crashes in log4j code - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: bin/hadoop dfs -rm <dir> crashes in log4j code\n\nDescription: When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages: log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory) at java.io.FileOutputStream.openAppend(Native Method) at java.io.FileOutputStream.(FileOutputStream.java:177) at java.io.FileOutputStream.(FileOutputStream.java:102) at org.apache.log4j.FileAppender.setFile(FileAppender.java:289) at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163) at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215) at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96) at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654) at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612) at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468) at org.apache.log4j.LogManager.(LogManager.java:122) at org.apache.log4j.Logger.getLogger(Logger.java:104) at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229) at org.apache.commons.logging.impl.Log4JLogger.(Log4JLogger.java:65) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constr", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'bin/hadoop dfs -rm <dir> crashes in log4j code' and how was it resolved?", "context": "Title: bin/hadoop dfs -rm <dir> crashes in log4j code\n\nDescription: When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages: log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory) at java.io.FileOutputStream.openAppend(Native Method) at java.io.FileOutputStream.(FileOutputStream.java:177) at java.io.FileOutputStream.(FileOutputStream.java:102) at org.apache.log4j.FileAppender.setFile(FileAppender.java:289) at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163) at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215) at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96) at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654) at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612) at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468) at org.apache.log4j.LogManager.(LogManager.java:122) at org.apache.log4j.Logger.getLogger(Logger.java:104) at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229) at org.apache.commons.logging.impl.Log4JLogger.(Log4JLogger.java:65) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constr", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-274", "project": "HADOOP", "title": "The new logging framework puts application logs into server directory in hadoop.log", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-06-05T23:26:26+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications). Thoughts?", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications). Thoughts?\n\nComments:", "output": "The new logging framework puts application logs into server directory in hadoop.log - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: The new logging framework puts application logs into server directory in hadoop.log\n\nDescription: The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications). Thoughts?", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'The new logging framework puts application logs into server directory in hadoop.log' and how was it resolved?", "context": "Title: The new logging framework puts application logs into server directory in hadoop.log\n\nDescription: The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications). Thoughts?", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-276", "project": "HADOOP", "title": "No appenders could be found for logger", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-06-06T04:56:07+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "When you start the servers with an old configuration directory without the properties files, you get messages about: log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration). log4j:WARN Please initialize the log4j system properly. The problem is that the property files are not included in the jar file.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "When you start the servers with an old configuration directory without the properties files, you get messages about: log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration). log4j:WARN Please initialize the log4j system properly. The problem is that the property files are not included in the jar file.\n\nComments:", "output": "No appenders could be found for logger - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: No appenders could be found for logger\n\nDescription: When you start the servers with an old configuration directory without the properties files, you get messages about: log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration). log4j:WARN Please initialize the log4j system properly. The problem is that the property files are not included in the jar file.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'No appenders could be found for logger' and how was it resolved?", "context": "Title: No appenders could be found for logger\n\nDescription: When you start the servers with an old configuration directory without the properties files, you get messages about: log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration). log4j:WARN Please initialize the log4j system properly. The problem is that the property files are not included in the jar file.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-277", "project": "HADOOP", "title": "Race condition in Configuration.getLocalPath()", "status": "Closed", "priority": "Major", "reporter": "Peter Sutter", "created": "2006-06-06T08:11:33+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Sameer Paranjpye", "resolution": "Fixed"}, "description": "(attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/map_15.out ... 060605 142531 task_0001_r_000009_1 0.31808624% reduce > append > /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142725 task_0001_r_000009_1 java.io.FileNotFoundException: /data2/tmp/mapred/local/task_0001_r_000009_1/map_15.out", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "(attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/map_15.out ... 060605 142531 task_0001_r_000009_1 0.31808624% reduce", "output": "Race condition in Configuration.getLocalPath() - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Race condition in Configuration.getLocalPath()\n\nDescription: (attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Race condition in Configuration.getLocalPath()' and how was it resolved?", "context": "Title: Race condition in Configuration.getLocalPath()\n\nDescription: (attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-30", "project": "HADOOP", "title": "DFS shell: support for ls -r and cat", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "created": "2006-02-10T08:14:53+00:00", "updated": "2006-08-03T17:46:27+00:00", "resolution": "Fixed"}, "description": "patch attached", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "patch attached\n\nComments:", "output": "DFS shell: support for ls -r and cat - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: DFS shell: support for ls -r and cat\n\nDescription: patch attached", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'DFS shell: support for ls -r and cat' and how was it resolved?", "context": "Title: DFS shell: support for ls -r and cat\n\nDescription: patch attached", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-34", "project": "HADOOP", "title": "Build Paths Relative to PWD in build.xml", "status": "Closed", "priority": "Trivial", "reporter": "Jeremy Bensley", "created": "2006-02-14T02:00:26+00:00", "updated": "2006-08-03T17:46:27+00:00", "resolution": "Fixed"}, "description": "In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear: BUILD FAILED /home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory I have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile. I am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear: BUILD FAILED /home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory I have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile. I am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.\n\nComments:", "output": "Build Paths Relative to PWD in build.xml - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Build Paths Relative to PWD in build.xml\n\nDescription: In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear: BUILD FAILED /home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory I have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile. I am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.", "output": "Priority: Trivial | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Build Paths Relative to PWD in build.xml' and how was it resolved?", "context": "Title: Build Paths Relative to PWD in build.xml\n\nDescription: In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear: BUILD FAILED /home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory I have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile. I am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-36", "project": "HADOOP", "title": "Adding some uniformity/convenience to environment management", "status": "Closed", "priority": "Major", "reporter": "Bryan Pendleton", "created": "2006-02-14T05:03:38+00:00", "updated": "2006-08-03T17:46:27+00:00", "resolution": "Fixed"}, "description": "Currently, \"slaves\" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves Perhaps split slaves, having a different set for \"datanodes\" vs. \"tasktracker\" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption. Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable. These changes would probably be useful to the nutch project, too.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Currently, \"slaves\" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves Perhaps split slaves, having a different set for \"datanodes\" vs. \"tasktracker\" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption. Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable. These changes would probably be useful to the nutch project, too.\n\nComments:", "output": "Adding some uniformity/convenience to environment management - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Adding some uniformity/convenience to environment management\n\nDescription: Currently, \"slaves\" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves Perhaps split slaves, having a different set for \"datanodes\" vs. \"tasktracker\" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption. Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable. These changes would probably be useful to the nutch project, too.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Adding some uniformity/convenience to environment management' and how was it resolved?", "context": "Title: Adding some uniformity/convenience to environment management\n\nDescription: Currently, \"slaves\" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves Perhaps split slaves, having a different set for \"datanodes\" vs. \"tasktracker\" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption. Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable. These changes would probably be useful to the nutch project, too.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-40", "project": "HADOOP", "title": "bufferSize argument is ignored in FileSystem.create(File, boolean, int)", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "created": "2006-02-18T07:43:11+00:00", "updated": "2006-08-03T17:46:28+00:00", "resolution": "Fixed"}, "description": "org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize) ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config. MapReduce although is probably affected by that, see org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done) The attached patch would fix it.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize) ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config. MapReduce although is probably affected by that, see org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done) The attached patch would fix it.\n\nComments:", "output": "bufferSize argument is ignored in FileSystem.create(File, boolean, int) - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: bufferSize argument is ignored in FileSystem.create(File, boolean, int)\n\nDescription: org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize) ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config. MapReduce although is probably affected by that, see org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done) The attached patch would fix it.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'bufferSize argument is ignored in FileSystem.create(File, boolean, int)' and how was it resolved?", "context": "Title: bufferSize argument is ignored in FileSystem.create(File, boolean, int)\n\nDescription: org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize) ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config. MapReduce although is probably affected by that, see org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done) The attached patch would fix it.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-41", "project": "HADOOP", "title": "JAVA_OPTS for the TaskRunner Child", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-02-18T09:47:33+00:00", "updated": "2006-08-03T17:46:28+00:00", "resolution": "Fixed"}, "description": "Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes. Examples of why its useful being able to set options are the child include: + Being able to set '-server' option or '-c64'. + Passing logging.properties to configure child logging. + Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process. Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes. Examples of why its useful being able to set options are the child include: + Being able to set '-server' option or '-c64'. + Passing logging.properties to configure child logging. + Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process. Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.\n\nComments:", "output": "JAVA_OPTS for the TaskRunner Child - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: JAVA_OPTS for the TaskRunner Child\n\nDescription: Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes. Examples of why its useful being able to set options are the child include: + Being able to set '-server' option or '-c64'. + Passing logging.properties to configure child logging. + Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process. Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'JAVA_OPTS for the TaskRunner Child' and how was it resolved?", "context": "Title: JAVA_OPTS for the TaskRunner Child\n\nDescription: Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes. Examples of why its useful being able to set options are the child include: + Being able to set '-server' option or '-c64'. + Passing logging.properties to configure child logging. + Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process. Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-42", "project": "HADOOP", "title": "PositionCache decrements its position for reads at the end of file", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "created": "2006-02-18T09:58:44+00:00", "updated": "2006-08-03T17:46:28+00:00", "resolution": "Fixed"}, "description": "See int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained. The attached patch would fix it.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "See int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained. The attached patch would fix it.\n\nComments:", "output": "PositionCache decrements its position for reads at the end of file - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: PositionCache decrements its position for reads at the end of file\n\nDescription: See int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained. The attached patch would fix it.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'PositionCache decrements its position for reads at the end of file' and how was it resolved?", "context": "Title: PositionCache decrements its position for reads at the end of file\n\nDescription: See int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained. The attached patch would fix it.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-44", "project": "HADOOP", "title": "RPC exceptions should include remote stack trace", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "created": "2006-02-22T07:06:46+00:00", "updated": "2006-08-03T17:46:28+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging.\n\nComments:", "output": "RPC exceptions should include remote stack trace - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: RPC exceptions should include remote stack trace\n\nDescription: Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'RPC exceptions should include remote stack trace' and how was it resolved?", "context": "Title: RPC exceptions should include remote stack trace\n\nDescription: Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-5", "project": "HADOOP", "title": "need commons-logging-api jar file", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "created": "2006-02-07T03:04:15+00:00", "updated": "2006-08-03T17:46:26+00:00", "resolution": "Fixed"}, "description": "The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.\n\nComments:", "output": "need commons-logging-api jar file - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: need commons-logging-api jar file\n\nDescription: The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'need commons-logging-api jar file' and how was it resolved?", "context": "Title: need commons-logging-api jar file\n\nDescription: The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-58", "project": "HADOOP", "title": "Hadoop requires configuration of hadoop-site.xml or won't run", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-02-24T03:43:08+00:00", "updated": "2006-08-03T17:46:29+00:00", "resolution": "Won't Fix"}, "description": "On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost. Currently this is not the case. Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'. It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties. Revision: 379930", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost. Currently this is not the case. Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'. It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties. Revision: 379930\n\nComments:", "output": "Hadoop requires configuration of hadoop-site.xml or won't run - Resolved: Won't Fix"}, "classification": {"task": "classification", "input": "Title: Hadoop requires configuration of hadoop-site.xml or won't run\n\nDescription: On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost. Currently this is not the case. Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'. It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties. Revision: 379930", "output": "Priority: Minor | Status: Closed | Resolution: Won't Fix"}, "qa": {"task": "qa", "question": "What is the issue with 'Hadoop requires configuration of hadoop-site.xml or won't run' and how was it resolved?", "context": "Title: Hadoop requires configuration of hadoop-site.xml or won't run\n\nDescription: On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost. Currently this is not the case. Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'. It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties. Revision: 379930", "answer": "The issue was resolved as: Won't Fix"}}}
{"metadata": {"issue_key": "HADOOP-6", "project": "HADOOP", "title": "missing build directory in classpath", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "created": "2006-02-07T03:19:03+00:00", "updated": "2006-08-03T17:46:26+00:00", "resolution": "Fixed"}, "description": "When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.\n\nComments:", "output": "missing build directory in classpath - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: missing build directory in classpath\n\nDescription: When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'missing build directory in classpath' and how was it resolved?", "context": "Title: missing build directory in classpath\n\nDescription: When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-60", "project": "HADOOP", "title": "Specification of alternate conf. directory", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-02-28T01:56:17+00:00", "updated": "2006-08-03T17:46:29+00:00", "resolution": "Fixed"}, "description": "Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct. Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct. Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.\n\nComments:", "output": "Specification of alternate conf. directory - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Specification of alternate conf. directory\n\nDescription: Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct. Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Specification of alternate conf. directory' and how was it resolved?", "context": "Title: Specification of alternate conf. directory\n\nDescription: Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct. Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-62", "project": "HADOOP", "title": "can't get environment variables from HADOOP_CONF_DIR", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "created": "2006-03-02T08:49:53+00:00", "updated": "2006-08-03T17:46:29+00:00", "assignee": "Owen O'Malley", "resolution": "Duplicate"}, "description": "The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.\n\nComments:", "output": "can't get environment variables from HADOOP_CONF_DIR - Resolved: Duplicate"}, "classification": {"task": "classification", "input": "Title: can't get environment variables from HADOOP_CONF_DIR\n\nDescription: The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.", "output": "Priority: Minor | Status: Closed | Resolution: Duplicate"}, "qa": {"task": "qa", "question": "What is the issue with 'can't get environment variables from HADOOP_CONF_DIR' and how was it resolved?", "context": "Title: can't get environment variables from HADOOP_CONF_DIR\n\nDescription: The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.", "answer": "The issue was resolved as: Duplicate"}}}
{"metadata": {"issue_key": "HADOOP-65", "project": "HADOOP", "title": "add a record I/O framework to hadoop", "status": "Closed", "priority": "Minor", "reporter": "Sameer Paranjpye", "created": "2006-03-07T07:12:40+00:00", "updated": "2006-08-03T17:46:29+00:00", "resolution": "Fixed"}, "description": "Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.\n\nComments:", "output": "add a record I/O framework to hadoop - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: add a record I/O framework to hadoop\n\nDescription: Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'add a record I/O framework to hadoop' and how was it resolved?", "context": "Title: add a record I/O framework to hadoop\n\nDescription: Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-78", "project": "HADOOP", "title": "rpc commands not buffered", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-03-14T06:48:27+00:00", "updated": "2006-08-03T17:46:31+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "Calls using Hadoop's RPC framework get sent across the network byte by byte.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Calls using Hadoop's RPC framework get sent across the network byte by byte.\n\nComments:", "output": "rpc commands not buffered - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: rpc commands not buffered\n\nDescription: Calls using Hadoop's RPC framework get sent across the network byte by byte.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'rpc commands not buffered' and how was it resolved?", "context": "Title: rpc commands not buffered\n\nDescription: Calls using Hadoop's RPC framework get sent across the network byte by byte.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-80", "project": "HADOOP", "title": "binary key", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-03-14T15:18:51+00:00", "updated": "2006-08-03T17:46:31+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "I needed a binary key type, so I extended BytesWritable to be comparable also.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "I needed a binary key type, so I extended BytesWritable to be comparable also.\n\nComments:", "output": "binary key - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: binary key\n\nDescription: I needed a binary key type, so I extended BytesWritable to be comparable also.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'binary key' and how was it resolved?", "context": "Title: binary key\n\nDescription: I needed a binary key type, so I extended BytesWritable to be comparable also.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-82", "project": "HADOOP", "title": "JobTracker loses it: NoSuchElementException", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-03-16T01:15:11+00:00", "updated": "2006-08-03T17:46:31+00:00", "resolution": "Fixed"}, "description": "On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log. 060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException java.io.IOException: java.util.NoSuchElementException at java.util.TreeMap.key(TreeMap.java:433) at java.util.TreeMap.firstKey(TreeMap.java:287) at java.util.TreeSet.first(TreeSet.java:407) at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read(BufferedInputStream.java:235) at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210) at java.io.DataInputStream.readInt(DataInputStream.java:353) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557) at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666) I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks. Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it: 060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org 060314 204758 Task 'task_m_4d6ht0' has been lost. 060314 204811 Adding task 'task_m_fb0wf0' to tip tip_fizr7m, for tracker 'tracker_70065' on ia109314.archive.org 060314 210118 Task 'task_m_fb0wf0' has been lost. 060314 210119 Adding task 'task_m_irar47' to tip tip_fizr7m, for tracker 'tracker_82285' on ia109324.archive.org 060314 211541 Taskid 'task_m_irar47' has finished successfully. 060314 211541 Task 'task_m_irar47' has completed. 060314 211543 Adding task 'task_m_qo1g69' to tip tip_fizr7m, for tracker 'tracker_97839' on ia109306.archive.org 060314 213004 Taskid 'task_m_qo1g69' has finished successfully. 060314 213004 Task 'task_m_qo1g69' has completed. 060314 213005 Adding task 'task_m_t0lnzk' to tip tip_fizr7m, for tracker 'tracker_57273' on ia109314.archive.org 060314 214118 Task 'task_m_t0lnzk' has been lost. So, we lose two, complete two, then lose a third. TIP should have been done on first completion. TIP accounting is off.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log. 060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException java.io.IOException: java.util.NoSuchElementException at java.util.TreeMap.key(TreeMap.java:433) at java.util.TreeMap.firstKey(TreeMap.java:287) at java.util.TreeSet.first(TreeSet.java:407) at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read(BufferedInputStream.java:235) at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210) at java.io.DataInputStream.readInt(DataInputStream.java:353) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557) at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666) I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks. Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it: 060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org 060314 204758 Task 'task_m_4d6ht0' has been lost. 060314 204811 Addi", "output": "JobTracker loses it: NoSuchElementException - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: JobTracker loses it: NoSuchElementException\n\nDescription: On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log. 060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException java.io.IOException: java.util.NoSuchElementException at java.util.TreeMap.key(TreeMap.java:433) at java.util.TreeMap.firstKey(TreeMap.java:287) at java.util.TreeSet.first(TreeSet.java:407) at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read(BufferedInputStream.java:235) at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210) at java.io.DataInputStream.readInt(DataInputStream.java:353) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557) at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666) I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks. Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it: 060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org 060", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'JobTracker loses it: NoSuchElementException' and how was it resolved?", "context": "Title: JobTracker loses it: NoSuchElementException\n\nDescription: On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log. 060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException java.io.IOException: java.util.NoSuchElementException at java.util.TreeMap.key(TreeMap.java:433) at java.util.TreeMap.firstKey(TreeMap.java:287) at java.util.TreeSet.first(TreeSet.java:407) at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read(BufferedInputStream.java:235) at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210) at java.io.DataInputStream.readInt(DataInputStream.java:353) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557) at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666) I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks. Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it: 060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org 060", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-86", "project": "HADOOP", "title": "If corrupted map outputs, reducers get stuck fetching forever", "status": "Closed", "priority": "Major", "reporter": "Michael Stack", "created": "2006-03-17T01:58:09+00:00", "updated": "2006-08-03T17:46:32+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException. Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside). And so it goes till the cows come home. Doug applied a patch that in map output file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again). This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts. A further patch that clears the severe flag was posted to the list. This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs. During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail). This issue covers implementing a better solution. Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT. A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates JT when corrupted map output.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException. Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside). And so it goes till the cows come home. Doug applied a patch that in map output file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again). This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts. A further patch that clears the severe flag was posted to the list. This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs. During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail). This issue covers implementing a better solution. Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT. A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates JT when corrupted map output.\n\nComments:", "output": "If corrupted map outputs, reducers get stuck fetching forever - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: If corrupted map outputs, reducers get stuck fetching forever\n\nDescription: In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException. Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside). And so it goes till the cows come home. Doug applied a patch that in map output file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again). This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts. A further patch that clears the severe flag was posted to the list. This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs. During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail). This issue covers implementing a better solution. Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT. A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates ", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'If corrupted map outputs, reducers get stuck fetching forever' and how was it resolved?", "context": "Title: If corrupted map outputs, reducers get stuck fetching forever\n\nDescription: In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException. Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside). And so it goes till the cows come home. Doug applied a patch that in map output file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again). This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts. A further patch that clears the severe flag was posted to the list. This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs. During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail). This issue covers implementing a better solution. Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT. A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates ", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-87", "project": "HADOOP", "title": "SequenceFile performance degrades substantially compression is on and large values are encountered", "status": "Closed", "priority": "Major", "reporter": "Sameer Paranjpye", "created": "2006-03-17T06:49:48+00:00", "updated": "2006-08-03T17:46:32+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "The code snippet in quesiton is: if (deflateValues) { deflateIn.reset(); val.write(deflateIn); deflater.reset(); deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength()); deflater.finish(); while (!deflater.finished()) { int count = deflater.deflate(deflateOut); buffer.write(deflateOut, 0, count); } } else { A couple of issues with this code: 1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem. 2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'. Proposed fix: 1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size. 2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, needs to shrink as well.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The code snippet in quesiton is: if (deflateValues) { deflateIn.reset(); val.write(deflateIn); deflater.reset(); deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength()); deflater.finish(); while (!deflater.finished()) { int count = deflater.deflate(deflateOut); buffer.write(deflateOut, 0, count); } } else { A couple of issues with this code: 1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem. 2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'. Proposed fix: 1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size. 2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, needs to shrink as well.\n\nComments:", "output": "SequenceFile performance degrades substantially compression is on and large values are encountered - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: SequenceFile performance degrades substantially compression is on and large values are encountered\n\nDescription: The code snippet in quesiton is: if (deflateValues) { deflateIn.reset(); val.write(deflateIn); deflater.reset(); deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength()); deflater.finish(); while (!deflater.finished()) { int count = deflater.deflate(deflateOut); buffer.write(deflateOut, 0, count); } } else { A couple of issues with this code: 1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem. 2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'. Proposed fix: 1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size. 2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, need", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'SequenceFile performance degrades substantially compression is on and large values are encountered' and how was it resolved?", "context": "Title: SequenceFile performance degrades substantially compression is on and large values are encountered\n\nDescription: The code snippet in quesiton is: if (deflateValues) { deflateIn.reset(); val.write(deflateIn); deflater.reset(); deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength()); deflater.finish(); while (!deflater.finished()) { int count = deflater.deflate(deflateOut); buffer.write(deflateOut, 0, count); } } else { A couple of issues with this code: 1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem. 2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'. Proposed fix: 1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size. 2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, need", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-125", "project": "HADOOP", "title": "LocalFileSystem.makeAbsolute bug on Windows", "status": "Closed", "priority": "Minor", "reporter": "Peter Sutter", "created": "2006-04-08T06:43:59+00:00", "updated": "2006-08-03T17:46:35+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop). Problem: if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails. Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three: (1) relative to current directory (subdir/file) (2) relative to current disk (/dir/subdir/file) (3) absolute (c:/dir/subdir/file) So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem: private File makeAbsolute(File f) { if (f.isAbsolute()) { return f; } else { return new File(workingDir, f.toString()); } } Im happy to explain if this explanation is confusing...", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop). Problem: if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails. Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three: (1) relative to current directory (subdir/file) (2) relative to current disk (/dir/subdir/file) (3) absolute (c:/dir/subdir/file) So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem: private File makeAbsolute(File f) { if (f.isAbsolute()) { return f; } else { return new File(workingDir, f.toString()); } } Im happy to explain if this explanation is confusing...\n\nComments:", "output": "LocalFileSystem.makeAbsolute bug on Windows - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: LocalFileSystem.makeAbsolute bug on Windows\n\nDescription: LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop). Problem: if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails. Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three: (1) relative to current directory (subdir/file) (2) relative to current disk (/dir/subdir/file) (3) absolute (c:/dir/subdir/file) So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem: private File makeAbsolute(File f) { if (f.isAbsolute()) { return f; } else { return new File(workingDir, f.toString()); } } Im happy to explain if this explanation is confusing...", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'LocalFileSystem.makeAbsolute bug on Windows' and how was it resolved?", "context": "Title: LocalFileSystem.makeAbsolute bug on Windows\n\nDescription: LocalFileSystem.makeAbsolute() has a bug when running on Windows (which is very useful for the development phase of a Hadoop task on one's laptop). Problem: if a pathname such as /tmp/hadoop... is given in a config file, when the jobconf file is created, it is put into the relative directory named: currentdir/tmp/hadoop..., but when hadoop tries to open the file, it looks in c:/tmp/hadoop..., and the job fails. Cause: while Unix has two kinds of filespecs (relative and absolute), WIndows actually has three: (1) relative to current directory (subdir/file) (2) relative to current disk (/dir/subdir/file) (3) absolute (c:/dir/subdir/file) So when a config file specifies a directory with what-is-on-unix an absolute path (/tmp/hadoop...), the makeAbsolute() method will not work correctly. Basically, File.isAbsolute() will return false for cases (1) and (2) above, but true for case (3), which is not expected by the code below. The solution would be to code explicit detection of all three casses for Windows in the code below from LocalFileSystem: private File makeAbsolute(File f) { if (f.isAbsolute()) { return f; } else { return new File(workingDir, f.toString()); } } Im happy to explain if this explanation is confusing...", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-129", "project": "HADOOP", "title": "FileSystem should not name files with java.io.File", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "created": "2006-04-11T03:05:09+00:00", "updated": "2006-08-03T17:46:35+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "In Hadoop's FileSystem API, files are currently named using java.io.File. This is confusing, as many methods on that class are inappropriate to call on Hadoop paths. For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file. Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names. For example, new File(\"/foo\") is not absolute on Windows, and prints its path as \"\\\\foo\", which causes confusion. To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI. The advantage of URI is that it can also naturally include the namenode host and port. The disadvantage is that URI does not support tree operations like getParent(). This change will cause a lot of incompatibility. Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "In Hadoop's FileSystem API, files are currently named using java.io.File. This is confusing, as many methods on that class are inappropriate to call on Hadoop paths. For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file. Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names. For example, new File(\"/foo\") is not absolute on Windows, and prints its path as \"\\\\foo\", which causes confusion. To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI. The advantage of URI is that it can also naturally include the namenode host and port. The disadvantage is that URI does not support tree operations like getParent(). This change will cause a lot of incompatibility. Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.\n\nComments:", "output": "FileSystem should not name files with java.io.File - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: FileSystem should not name files with java.io.File\n\nDescription: In Hadoop's FileSystem API, files are currently named using java.io.File. This is confusing, as many methods on that class are inappropriate to call on Hadoop paths. For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file. Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names. For example, new File(\"/foo\") is not absolute on Windows, and prints its path as \"\\\\foo\", which causes confusion. To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI. The advantage of URI is that it can also naturally include the namenode host and port. The disadvantage is that URI does not support tree operations like getParent(). This change will cause a lot of incompatibility. Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'FileSystem should not name files with java.io.File' and how was it resolved?", "context": "Title: FileSystem should not name files with java.io.File\n\nDescription: In Hadoop's FileSystem API, files are currently named using java.io.File. This is confusing, as many methods on that class are inappropriate to call on Hadoop paths. For example, calling isDirectory(), exists(), etc. on a java.io.File is not the same as calling FileSystem.isDirectory() or FileSystem.exists() passing that same file. Using java.io.File also makes correct operation on Windows difficult, since java.io.File operates differently on Windows in order to accomodate Windows path names. For example, new File(\"/foo\") is not absolute on Windows, and prints its path as \"\\\\foo\", which causes confusion. To fix this we could replace the uses of java.io.File in the FileSystem API with String, a new FileName class, or perhaps java.net.URI. The advantage of URI is that it can also naturally include the namenode host and port. The disadvantage is that URI does not support tree operations like getParent(). This change will cause a lot of incompatibility. Thus it should probably be made early in a development cycle in order to maximize the time for folks to adapt to it.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-132", "project": "HADOOP", "title": "An API for reporting performance metrics", "status": "Closed", "priority": "Major", "reporter": "David Bowen", "created": "2006-04-14T03:12:36+00:00", "updated": "2006-08-03T17:46:35+00:00", "resolution": "Fixed"}, "description": "I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so. The idea is for the API to be sufficiently abstract that various different implementations can be plugged in. In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia. It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network. I'd be very interested in people's thoughts about what the requirements should be for such an API. - David Bowen", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so. The idea is for the API to be sufficiently abstract that various different implementations can be plugged in. In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia. It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network. I'd be very interested in people's thoughts about what the requirements should be for such an API. - David Bowen\n\nComments:", "output": "An API for reporting performance metrics - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: An API for reporting performance metrics\n\nDescription: I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so. The idea is for the API to be sufficiently abstract that various different implementations can be plugged in. In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia. It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network. I'd be very interested in people's thoughts about what the requirements should be for such an API. - David Bowen", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'An API for reporting performance metrics' and how was it resolved?", "context": "Title: An API for reporting performance metrics\n\nDescription: I'd like to propose adding an API for reporting performance metrics. I will post some javadoc as soon as I figure out how to do so. The idea is for the API to be sufficiently abstract that various different implementations can be plugged in. In particular, there would be one that just writes the metric data to a file, and another that sends metrics to Ganglia. It would also be possible to plug in an implementation that can support high-frequency (say, per-second) sending of fairly large amounts of data (up to hundreds of metrics) across the network. I'd be very interested in people's thoughts about what the requirements should be for such an API. - David Bowen", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-139", "project": "HADOOP", "title": "Deadlock in LocalFileSystem lock/release", "status": "Closed", "priority": "Major", "reporter": "Igor Bolotin", "created": "2006-04-15T10:39:58+00:00", "updated": "2006-08-03T17:46:36+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 1. First thread locks the file and starts some long-running process. 2. Second thread tries to lock the file and it blocks inside channel lock method. It keeps LocalFileSystem instance \"locked\" as well. 3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is \"locked\" by second thread - both threads are waiting to each other.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 1. First thread locks the file and starts some long-running process. 2. Second thread tries to lock the file and it blocks inside channel lock method. It keeps LocalFileSystem instance \"locked\" as well. 3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is \"locked\" by second thread - both threads are waiting to each other.\n\nComments:", "output": "Deadlock in LocalFileSystem lock/release - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Deadlock in LocalFileSystem lock/release\n\nDescription: LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 1. First thread locks the file and starts some long-running process. 2. Second thread tries to lock the file and it blocks inside channel lock method. It keeps LocalFileSystem instance \"locked\" as well. 3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is \"locked\" by second thread - both threads are waiting to each other.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Deadlock in LocalFileSystem lock/release' and how was it resolved?", "context": "Title: Deadlock in LocalFileSystem lock/release\n\nDescription: LocalFileSystem lock/release methods marked synchronized and inside they lock file channel - this produces deadlock situation. Let's see how it happens: 1. First thread locks the file and starts some long-running process. 2. Second thread tries to lock the file and it blocks inside channel lock method. It keeps LocalFileSystem instance \"locked\" as well. 3. First thread finished it's processing and tries to release lock - it blocks because LocalFileSystem instance is \"locked\" by second thread - both threads are waiting to each other.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-149", "project": "HADOOP", "title": "TaskTracker#unJar trashes file modes", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-04-20T00:59:20+00:00", "updated": "2006-08-03T17:46:36+00:00", "resolution": "Won't Fix"}, "description": "Last Changed Rev: 395069 The unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves. I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry. I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it). Perhaps support for jobs as tar(.gz) bundles? (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Last Changed Rev: 395069 The unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves. I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry. I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it). Perhaps support for jobs as tar(.gz) bundles? (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).\n\nComments:", "output": "TaskTracker#unJar trashes file modes - Resolved: Won't Fix"}, "classification": {"task": "classification", "input": "Title: TaskTracker#unJar trashes file modes\n\nDescription: Last Changed Rev: 395069 The unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves. I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry. I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it). Perhaps support for jobs as tar(.gz) bundles? (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).", "output": "Priority: Minor | Status: Closed | Resolution: Won't Fix"}, "qa": {"task": "qa", "question": "What is the issue with 'TaskTracker#unJar trashes file modes' and how was it resolved?", "context": "Title: TaskTracker#unJar trashes file modes\n\nDescription: Last Changed Rev: 395069 The unJar'ing of the job 'jar', trashes any file modes I've lovingly set at zip time. This is a bit of a pain when my job wants to run external scripts and I want to bundle the scripts up in the jar itself for distribution out to slaves. I ain't sure how to address the issue though. Nought about unix file modes in JarEntry nor ZipEntry. I tried the ant 1.6.5 task unjar and unzip tasks thinking they'd respect file modes but they do same as TaskTracker#unJar (Commons zip makes mention of unix file modes but I haven't tried it). Perhaps support for jobs as tar(.gz) bundles? (But again, couldn't use ant to untar. It does same as unzip/unjar trashing file permissions).", "answer": "The issue was resolved as: Won't Fix"}}}
{"metadata": {"issue_key": "HADOOP-151", "project": "HADOOP", "title": "RPC code has socket leak?", "status": "Closed", "priority": "Major", "reporter": "Peter Sutter", "created": "2006-04-20T05:18:25+00:00", "updated": "2006-08-03T17:46:36+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below. The comment above the declaration was a bit of giveaway: //TODO mb@media-style.com: static client or non-static client? private static Client CLIENT; private static class Invoker implements InvocationHandler { private InetSocketAddress address; public Invoker(InetSocketAddress address, Configuration conf) { this.address = address; CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ObjectWritable value = (ObjectWritable) CLIENT.call(new Invocation(method, args), address); return value.get(); } } /** Construct a client-side proxy object that implements the named protocol, * talking to a server at the named address. */ public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) { return Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] { protocol }, new Invoker(addr, conf)); } /** Expert: Make multiple, parallel calls to a set of servers. */ public static Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs, Configuration conf) throws IOException { Invocation[] invocations = new Invocation[params.length]; for (int i = 0; i < params.length; i++) invocations[i] = new Invocation(method, params[i]); CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } Writable[] wrappedValues = CLIENT.call(invocations, addrs); if (method.getReturnType() == Void.TYPE) { return null; } Object[] values = (Object[])Array.newInstance(method.getReturnType(),wrappedValues.length); for (int i = 0; i < values.length; i++) if (wrappedValues[i] != null) values[i] = ((ObjectWritable)wrappedValues[i]).get(); return values; }.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below. The comment above the declaration was a bit of giveaway: //TODO mb@media-style.com: static client or non-static client? private static Client CLIENT; private static class Invoker implements InvocationHandler { private InetSocketAddress address; public Invoker(InetSocketAddress address, Configuration conf) { this.address = address; CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ObjectWritable value = (ObjectWritable) CLIENT.call(new Invocation(method, args), address); return value.get(); } } /** Construct a client-side proxy object that implements the named protocol, * talking to a server at the named address. */ public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) { return Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] { protocol }, new Invoker(addr, conf)); } /** Expert: Make multiple, parallel calls to a set of servers. */ public static Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs, Configuration conf) throws IOException { Invocation[] invocations = new Invocation[params.length]; for (int i = 0; i < params.length; i++) invocations[i] = new Invocation(method, params[i]); CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } Writable[] wrappedValues = CLIENT.call(invocations, addrs); if (method.getReturnType() == Void.TYPE) { return null; } Object[] values = (Object[])Array.newInstance(method.getReturnType(),wrappedValues.length); ", "output": "RPC code has socket leak? - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: RPC code has socket leak?\n\nDescription: In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below. The comment above the declaration was a bit of giveaway: //TODO mb@media-style.com: static client or non-static client? private static Client CLIENT; private static class Invoker implements InvocationHandler { private InetSocketAddress address; public Invoker(InetSocketAddress address, Configuration conf) { this.address = address; CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ObjectWritable value = (ObjectWritable) CLIENT.call(new Invocation(method, args), address); return value.get(); } } /** Construct a client-side proxy object that implements the named protocol, * talking to a server at the named address. */ public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) { return Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] { protocol }, new Invoker(addr, conf)); } /** Expert: Make multiple, parallel calls to a set of servers. */ public static Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs, Configuration conf) throws IOException { Invocation[] invocations = new Invocation[params.length]; for (int i = 0; i < params.length; i++) invocations[i] = new Invocation(method, params[i]); CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } Writable[] wrappedValues = CLIENT.call(invocations, addrs); if (method.getReturnType() == Void.TYPE) { return null; } Object[] values = (Object[])Array.newInstance", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'RPC code has socket leak?' and how was it resolved?", "context": "Title: RPC code has socket leak?\n\nDescription: In RPC.java, the field named CLIENT should be neither static, nor a field of RPC. It should be (a) a private nonstatic field of InvocationHandler(),and (just further down), (b) a local variable in the RPC.call() method below. The comment above the declaration was a bit of giveaway: //TODO mb@media-style.com: static client or non-static client? private static Client CLIENT; private static class Invoker implements InvocationHandler { private InetSocketAddress address; public Invoker(InetSocketAddress address, Configuration conf) { this.address = address; CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } } public Object invoke(Object proxy, Method method, Object[] args) throws Throwable { ObjectWritable value = (ObjectWritable) CLIENT.call(new Invocation(method, args), address); return value.get(); } } /** Construct a client-side proxy object that implements the named protocol, * talking to a server at the named address. */ public static Object getProxy(Class protocol, InetSocketAddress addr, Configuration conf) { return Proxy.newProxyInstance(protocol.getClassLoader(), new Class[] { protocol }, new Invoker(addr, conf)); } /** Expert: Make multiple, parallel calls to a set of servers. */ public static Object[] call(Method method, Object[][] params, InetSocketAddress[] addrs, Configuration conf) throws IOException { Invocation[] invocations = new Invocation[params.length]; for (int i = 0; i < params.length; i++) invocations[i] = new Invocation(method, params[i]); CLIENT = (Client) conf.getObject(Client.class.getName()); if(CLIENT == null) { CLIENT = new Client(ObjectWritable.class, conf); conf.setObject(Client.class.getName(), CLIENT); } Writable[] wrappedValues = CLIENT.call(invocations, addrs); if (method.getReturnType() == Void.TYPE) { return null; } Object[] values = (Object[])Array.newInstance", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-160", "project": "HADOOP", "title": "sleeping with locks held", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-04-23T07:01:20+00:00", "updated": "2006-08-03T17:46:37+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.\n\nComments:", "output": "sleeping with locks held - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: sleeping with locks held\n\nDescription: I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'sleeping with locks held' and how was it resolved?", "context": "Title: sleeping with locks held\n\nDescription: I ran findbugs and it reported 7 cases of sleeping with locks held. Part of what is killing jobs is unreasonably slow responses from the servers (and task tracker in particular), and this may be contributing to that.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-166", "project": "HADOOP", "title": "IPC is unable to invoke methods that use interfaces as parameter", "status": "Closed", "priority": "Minor", "reporter": "Stefan Groschupf", "created": "2006-04-26T06:12:25+00:00", "updated": "2006-08-03T17:46:37+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.\n\nComments:", "output": "IPC is unable to invoke methods that use interfaces as parameter - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: IPC is unable to invoke methods that use interfaces as parameter\n\nDescription: Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'IPC is unable to invoke methods that use interfaces as parameter' and how was it resolved?", "context": "Title: IPC is unable to invoke methods that use interfaces as parameter\n\nDescription: Methods of the implementation class are searched via method name and call parameters that can be implementations of the interfaces defined in the method signature.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-167", "project": "HADOOP", "title": "reducing the number of Configuration & JobConf objects created", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-04-26T10:52:22+00:00", "updated": "2006-08-03T17:46:37+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.\n\nComments:", "output": "reducing the number of Configuration & JobConf objects created - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: reducing the number of Configuration & JobConf objects created\n\nDescription: Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'reducing the number of Configuration & JobConf objects created' and how was it resolved?", "context": "Title: reducing the number of Configuration & JobConf objects created\n\nDescription: Currently, Configuration and JobConf objects are created many times during executing a job. In particular, the Task Tracker creates a lot of them. They both clutter up the logs and parse the xml config files over and over again.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-17", "project": "HADOOP", "title": "tool to mount ndfs on linux", "status": "Closed", "priority": "Major", "reporter": "John Xing", "created": "2006-02-03T15:59:31+00:00", "updated": "2006-08-03T17:46:25+00:00", "resolution": "Duplicate"}, "description": "tool to mount ndfs on linux. It depends on fuse and fuse-j.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "tool to mount ndfs on linux. It depends on fuse and fuse-j.\n\nComments:", "output": "tool to mount ndfs on linux - Resolved: Duplicate"}, "classification": {"task": "classification", "input": "Title: tool to mount ndfs on linux\n\nDescription: tool to mount ndfs on linux. It depends on fuse and fuse-j.", "output": "Priority: Major | Status: Closed | Resolution: Duplicate"}, "qa": {"task": "qa", "question": "What is the issue with 'tool to mount ndfs on linux' and how was it resolved?", "context": "Title: tool to mount ndfs on linux\n\nDescription: tool to mount ndfs on linux. It depends on fuse and fuse-j.", "answer": "The issue was resolved as: Duplicate"}}}
{"metadata": {"issue_key": "HADOOP-172", "project": "HADOOP", "title": "rpc doesn't handle returning null for a String[]", "status": "Closed", "priority": "Blocker", "reporter": "Owen O'Malley", "created": "2006-04-28T00:15:15+00:00", "updated": "2006-08-03T17:46:38+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "The job tracker gets errors in returning the result from pollForTaskWithClosedJob 060427 100434 Served: pollForTaskWithClosedJob 0 declaredClass = [Ljava.lang.String; instance class = org.apache.hadoop.io.NullWritable at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95) at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The job tracker gets errors in returning the result from pollForTaskWithClosedJob 060427 100434 Served: pollForTaskWithClosedJob 0 declaredClass = [Ljava.lang.String; instance class = org.apache.hadoop.io.NullWritable at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95) at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)\n\nComments:", "output": "rpc doesn't handle returning null for a String[] - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: rpc doesn't handle returning null for a String[]\n\nDescription: The job tracker gets errors in returning the result from pollForTaskWithClosedJob 060427 100434 Served: pollForTaskWithClosedJob 0 declaredClass = [Ljava.lang.String; instance class = org.apache.hadoop.io.NullWritable at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95) at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)", "output": "Priority: Blocker | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'rpc doesn't handle returning null for a String[]' and how was it resolved?", "context": "Title: rpc doesn't handle returning null for a String[]\n\nDescription: The job tracker gets errors in returning the result from pollForTaskWithClosedJob 060427 100434 Served: pollForTaskWithClosedJob 0 declaredClass = [Ljava.lang.String; instance class = org.apache.hadoop.io.NullWritable at org.apache.hadoop.io.ObjectWritable.writeObject(ObjectWritable.java:95) at org.apache.hadoop.io.ObjectWritable.write(ObjectWritable.java:65) at org.apache.hadoop.ipc.Server$Handler.run(Server.java:230)", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-184", "project": "HADOOP", "title": "hadoop nightly build and regression test on a cluster", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "created": "2006-05-02T04:38:42+00:00", "updated": "2006-08-03T17:46:39+00:00", "assignee": "Mahadev Konar", "resolution": "Fixed"}, "description": "create a jar file for the tests and have filesystem and mapreduce tests on the cluster", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "create a jar file for the tests and have filesystem and mapreduce tests on the cluster\n\nComments:", "output": "hadoop nightly build and regression test on a cluster - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: hadoop nightly build and regression test on a cluster\n\nDescription: create a jar file for the tests and have filesystem and mapreduce tests on the cluster", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'hadoop nightly build and regression test on a cluster' and how was it resolved?", "context": "Title: hadoop nightly build and regression test on a cluster\n\nDescription: create a jar file for the tests and have filesystem and mapreduce tests on the cluster", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-190", "project": "HADOOP", "title": "Job fails though task succeeded if we fail to exit", "status": "Closed", "priority": "Major", "reporter": "Michael Stack", "created": "2006-05-03T04:28:56+00:00", "updated": "2006-08-03T17:46:39+00:00", "resolution": "Fixed"}, "description": "This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal. My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running, my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running. After ten minutes, TT steps in, kills the child and does cleanup of the successful output. Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails. Below is illustration of the problem using log output: .... 060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a um.jpg 24891 image/jpeg 060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp 24891 image/jpeg 060501 090401 Task task_0001_m_000798_0 is done. ... 060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds Killing. .... 060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0 060501 091410 task_0001_m_000798_0 done; removing files. Then, subsequently.... 060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa -12.out java.io.FileNotFoundException: LocalFS ... and on and on.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal. My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running, my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running. After ten minutes, TT steps in, kills the child and does cleanup of the successful output. Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails. Below is illustration of the problem using log output: .... 060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a um.jpg 24891 image/jpeg 060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp 24891 image/jpeg 060501 090401 Task task_0001_m_000798_0 is done. ... 060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds Killing. .... 060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0 060501 091410 task_0001_m_000798_0 done; removing files. Then, subsequently.... 060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa -12.out java.io.FileNotFoundException: LocalFS ... and on and on.\n\nComments:", "output": "Job fails though task succeeded if we fail to exit - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Job fails though task succeeded if we fail to exit\n\nDescription: This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal. My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running, my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running. After ten minutes, TT steps in, kills the child and does cleanup of the successful output. Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails. Below is illustration of the problem using log output: .... 060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a um.jpg 24891 image/jpeg 060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp 24891 image/jpeg 060501 090401 Task task_0001_m_000798_0 is done. ... 060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds Killing. .... 060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0 060501 091410 task_0001_m_000798_0 done; removing files. Then, subsequently.... 060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa -12.out java.io.FileNotFoundException: LocalFS ... and on and on.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Job fails though task succeeded if we fail to exit' and how was it resolved?", "context": "Title: Job fails though task succeeded if we fail to exit\n\nDescription: This is an odd case. Main cause will be programmer error but I suppose it could happen during normal processing. Whichever, would be grand if hadoop was better able to deal. My map task completed 'successfully' but because I had started threads inside in my task that were not set to be of daemon type that under certain circumstances were left running, my child stuck around after reporting 'done' -- the JVM wouldn't go down while non-daemon threads still running. After ten minutes, TT steps in, kills the child and does cleanup of the successful output. Because JT has been told the task completed successfully, reducers keep showing up looking for the output now removed -- until the job fails. Below is illustration of the problem using log output: .... 060501 090401 task_0001_m_000798_0 0.99491096% adding http://www.score.umd.edu/a um.jpg 24891 image/jpeg 060501 090401 task_0001_m_000798_0 1.0% adding http://www.score.umd.edu/album.jp 24891 image/jpeg 060501 090401 Task task_0001_m_000798_0 is done. ... 060501 091410 task_0001_m_000798_0: Task failed to report status for 608 seconds Killing. .... 060501 091410 Calling cleanup because was killed or FAILED task_0001_m_000798_0 060501 091410 task_0001_m_000798_0 done; removing files. Then, subsequently.... 060501 091422 SEVERE Can't open map output:/1/hadoop/tmp/task_0001_m_000798_0/pa -12.out java.io.FileNotFoundException: LocalFS ... and on and on.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-191", "project": "HADOOP", "title": "add hadoopStreaming to src/contrib", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "created": "2006-05-03T05:10:00+00:00", "updated": "2006-08-03T17:46:40+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks. The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce) TO test the patch: Merge the patch. The only existing file that is modified is trunk/build.xml trunk>ant deploy-contrib trunk>bin/hadoopStreaming : should show usage message trunk>ant test-contrib : should run one test successfully TO add src/contrib/someOtherProject: edit src/contrib/build.xml", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks. The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce) TO test the patch: Merge the patch. The only existing file that is modified is trunk/build.xml trunk>ant deploy-contrib trunk>bin/hadoopStreaming : should show usage message trunk>ant test-contrib : should run one test successfully TO add src/contrib/someOtherProject: edit src/contrib/build.xml\n\nComments:", "output": "add hadoopStreaming to src/contrib - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: add hadoopStreaming to src/contrib\n\nDescription: This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks. The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce) TO test the patch: Merge the patch. The only existing file that is modified is trunk/build.xml trunk>ant deploy-contrib trunk>bin/hadoopStreaming : should show usage message trunk>ant test-contrib : should run one test successfully TO add src/contrib/someOtherProject: edit src/contrib/build.xml", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'add hadoopStreaming to src/contrib' and how was it resolved?", "context": "Title: add hadoopStreaming to src/contrib\n\nDescription: This is a patch that adds a src/contrib/hadoopStreaming directory to the source tree. hadoopStreaming is a bridge to run non-Java code as Map/Reduce tasks. The unit test TestStreaming runs the Unix tools tr (as Map) and uniq (as Reduce) TO test the patch: Merge the patch. The only existing file that is modified is trunk/build.xml trunk>ant deploy-contrib trunk>bin/hadoopStreaming : should show usage message trunk>ant test-contrib : should run one test successfully TO add src/contrib/someOtherProject: edit src/contrib/build.xml", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-194", "project": "HADOOP", "title": "Distributed checkup of the file system consistency.", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "created": "2006-05-04T09:11:48+00:00", "updated": "2006-08-03T17:46:40+00:00", "assignee": "Konstantin Shvachko", "resolution": "Fixed"}, "description": "This is a map-reduce based test that checks consistency of the file system by reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions. This could be an alternative to the sequential checkup in dfsck. It would be nice to integrate distributed checkup with dfsck, but I don't yet see how. This test reuses classes defined in HADOOP-193.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "This is a map-reduce based test that checks consistency of the file system by reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions. This could be an alternative to the sequential checkup in dfsck. It would be nice to integrate distributed checkup with dfsck, but I don't yet see how. This test reuses classes defined in HADOOP-193.\n\nComments:", "output": "Distributed checkup of the file system consistency. - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Distributed checkup of the file system consistency.\n\nDescription: This is a map-reduce based test that checks consistency of the file system by reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions. This could be an alternative to the sequential checkup in dfsck. It would be nice to integrate distributed checkup with dfsck, but I don't yet see how. This test reuses classes defined in HADOOP-193.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Distributed checkup of the file system consistency.' and how was it resolved?", "context": "Title: Distributed checkup of the file system consistency.\n\nDescription: This is a map-reduce based test that checks consistency of the file system by reading all blocks of all files, and detecting which of them are missing or corrupted. See HADOOP-95 and HADOOP-101 for related discussions. This could be an alternative to the sequential checkup in dfsck. It would be nice to integrate distributed checkup with dfsck, but I don't yet see how. This test reuses classes defined in HADOOP-193.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-198", "project": "HADOOP", "title": "adding owen's examples to exampledriver", "status": "Closed", "priority": "Minor", "reporter": "Mahadev Konar", "created": "2006-05-05T07:33:24+00:00", "updated": "2006-08-03T17:46:40+00:00", "assignee": "Mahadev Konar", "resolution": "Fixed"}, "description": "owen's sorter and randomwriter are not added to the examples.jar file", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "owen's sorter and randomwriter are not added to the examples.jar file\n\nComments:", "output": "adding owen's examples to exampledriver - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: adding owen's examples to exampledriver\n\nDescription: owen's sorter and randomwriter are not added to the examples.jar file", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'adding owen's examples to exampledriver' and how was it resolved?", "context": "Title: adding owen's examples to exampledriver\n\nDescription: owen's sorter and randomwriter are not added to the examples.jar file", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-209", "project": "HADOOP", "title": "Add a program to recursively copy directories across file systems", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "created": "2006-05-11T06:05:18+00:00", "updated": "2006-08-03T17:46:41+00:00", "resolution": "Fixed"}, "description": "A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as: hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir \"cp\" command would invoke a map-reduce program to copy files recursively. I willl attach a patch as soon as svn is up and running.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as: hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir \"cp\" command would invoke a map-reduce program to copy files recursively. I willl attach a patch as soon as svn is up and running.\n\nComments:", "output": "Add a program to recursively copy directories across file systems - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Add a program to recursively copy directories across file systems\n\nDescription: A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as: hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir \"cp\" command would invoke a map-reduce program to copy files recursively. I willl attach a patch as soon as svn is up and running.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Add a program to recursively copy directories across file systems' and how was it resolved?", "context": "Title: Add a program to recursively copy directories across file systems\n\nDescription: A useful feature would be a simple command to copy directories recursively across filesystems. The source and destination path should be specified using a filesystem-neutral URI, such as: hadoop cp dfs://namenode1:port1/path/to/srcdir file:///path/to/local/destination/dir \"cp\" command would invoke a map-reduce program to copy files recursively. I willl attach a patch as soon as svn is up and running.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-211", "project": "HADOOP", "title": "logging improvements for Hadoop", "status": "Closed", "priority": "Minor", "reporter": "Sameer Paranjpye", "created": "2006-05-12T06:16:09+00:00", "updated": "2006-08-03T17:46:41+00:00", "assignee": "Sameer Paranjpye", "resolution": "Fixed"}, "description": "Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 broad changes to the way logging is currently done, these being: - The use of a uniform logging format by all Hadoop subsystems - The use of Apache commons logging as a facade above an underlying logging framework - The use of Log4J as the underlying logging framework instead of java.util.logging This is largely polishing work, but it seems like it would make log analysis and debugging easier in the short term. In the long term, it would future proof logging to the extent of allowing the logging framework used to change while requiring minimal code change. The propos changes are motivated by the following requirements which we think Hadoops logging should meet: - Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc. - Log entries should be clearly annotated with a timestamp and a logging level - Log entries should be traceable to the subsystem from which they originated - The logging implementation should allow log entries to be annotated with source code location information like classname, methodname, file and line number, without requiring code changes - It should be possible to change the logging implementation used without having to change thousands of lines of code - The mapping of loggers to destinations (files, directories, servers etc.) should be specified and modifiable via configuration Uniform logging format: All Hadoop logs should have the following structure. \\n \\n [\\n] . . . where the header line specifies the format of each log entry. The header line has the format: '# ...\\n'. The default format of each log entry is: '# Timestamp Level LoggerName Message', where: - Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS - Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.) - LoggerName is the short name of the logging subsystem from which the message originated e.g. fs.FSNamesystem, dfs.Datanode etc. - Message is the log message produced Why Apache commons logging and Log4J? Apache commons logging is a facade meant to be used as a wrapper around an underlying logging implementation. Bridges from Apache commons logging to popular logging implementations (Java logging, Log4J, Avalon etc.) are implemented and available as part of the commons logging distribution. Implementing a bridge to an unsupported implementation is fairly striaghtforward and involves the implementation of subclasses of the commons logging LogFactory and Logger classes. Using Apache commons logging and making all logging calls through it enables us to move to a different logging implementation by simply changing configuration in the best case. Even otherwise, it incurs minimal code churn overhead. Log4J offers a few benefits over java.util.logging that make it a more desirable choice for the logging back end. - Configuration Flexibility: The mapping of loggers to destinations (files, sockets etc.) can be completely specified in configuration. It is possible to do this with Java logging as well, however, configuration is a lot more restrictive. For instance, with Java logging all log files must have names derived from the same pattern. For the namenode, log files could be named with the pattern \"%h/namenode%u.log\" which would put log files in the user.home directory with names like namenode0.log etc. With Log4J it would be possible to configure the namenode to emit log files with different names, say heartbeats.log, namespace.log, clients.log etc. Configuration variables in Log4J can also have the values of system properties embedded in them. - Takes wrappers into account: Log4J takes into account the possibility that an application may be invoking it via a wrapper, such as Apache commons logging. This is important because logging event objects must be able to infer the context of the logging call such as classname, methodname etc. Inferring context is a relatively expensive operation that involves creating an exception and examining the stack trace to find the frame just before the first frame of the logging framework. It is therefore done lazily only when this information actually needs to be logged. Log4J can be instructed to look for the frame corresponding to the wrapper class, Java logging cannot. In the case of Java logging this means that a) the bridge from Apache commons logging is responsible for inferring the calling context and setting it in the logging event and b) this inference has to be done on every logging call regardless of whether or not it is needed. - More handy features: Log4J has some handy features that Java logging doesn't. A couple of examples of these: a) Date based rolling of log files b) Format control through configuration. Log4J has a PatternLayout class that can be configured to generate logs with a user specified pattern. The logging format described above can be described as \"%d{MM/dd/yyyy:HH:mm:SS} %c{2} %p %m\". The format specifiers indicate that each log line should have the date and time followed by the logger name followed by the logging level or priority followed by the application generated message.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 broad changes to the way logging is currently done, these being: - The use of a uniform logging format by all Hadoop subsystems - The use of Apache commons logging as a facade above an underlying logging framework - The use of Log4J as the underlying logging framework instead of java.util.logging This is largely polishing work, but it seems like it would make log analysis and debugging easier in the short term. In the long term, it would future proof logging to the extent of allowing the logging framework used to change while requiring minimal code change. The propos changes are motivated by the following requirements which we think Hadoops logging should meet: - Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc. - Log entries should be clearly annotated with a timestamp and a logging level - Log entries should be traceable to the subsystem from which they originated - The logging implementation should allow log entries to be annotated with source code location information like classname, methodname, file and line number, without requiring code changes - It should be possible to change the logging implementation used without having to change thousands of lines of code - The mapping of loggers to destinations (files, directories, servers etc.) should be specified and modifiable via configuration Uniform logging format: All Hadoop logs should have the following structure. \\n \\n [\\n] . . . where the header line specifies the format of each log entry. The header line has the format: '# ...\\n'. The default format of each log entry is: '# Timestamp Level LoggerName Message', where: - Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS - Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.) - LoggerName is the short name of the logging subsystem from which the message originated e.g. fs.FSNamesystem, dfs.Datanode etc. - Message is the log messa", "output": "logging improvements for Hadoop - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: logging improvements for Hadoop\n\nDescription: Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 broad changes to the way logging is currently done, these being: - The use of a uniform logging format by all Hadoop subsystems - The use of Apache commons logging as a facade above an underlying logging framework - The use of Log4J as the underlying logging framework instead of java.util.logging This is largely polishing work, but it seems like it would make log analysis and debugging easier in the short term. In the long term, it would future proof logging to the extent of allowing the logging framework used to change while requiring minimal code change. The propos changes are motivated by the following requirements which we think Hadoops logging should meet: - Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc. - Log entries should be clearly annotated with a timestamp and a logging level - Log entries should be traceable to the subsystem from which they originated - The logging implementation should allow log entries to be annotated with source code location information like classname, methodname, file and line number, without requiring code changes - It should be possible to change the logging implementation used without having to change thousands of lines of code - The mapping of loggers to destinations (files, directories, servers etc.) should be specified and modifiable via configuration Uniform logging format: All Hadoop logs should have the following structure. \\n \\n [\\n] . . . where the header line specifies the format of each log entry. The header line has the format: '# ...\\n'. The default format of each log entry is: '# Timestamp Level LoggerName Message', where: - Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS - Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.) - LoggerName is the short name of the logging subsystem from which the message originated e.g. fs.FSNam", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'logging improvements for Hadoop' and how was it resolved?", "context": "Title: logging improvements for Hadoop\n\nDescription: Here's a proposal for some impovements to the way Hadoop does logging. It advocates 3 broad changes to the way logging is currently done, these being: - The use of a uniform logging format by all Hadoop subsystems - The use of Apache commons logging as a facade above an underlying logging framework - The use of Log4J as the underlying logging framework instead of java.util.logging This is largely polishing work, but it seems like it would make log analysis and debugging easier in the short term. In the long term, it would future proof logging to the extent of allowing the logging framework used to change while requiring minimal code change. The propos changes are motivated by the following requirements which we think Hadoops logging should meet: - Hadoops logs should be amenable to analysis by tools like grep, sed, awk etc. - Log entries should be clearly annotated with a timestamp and a logging level - Log entries should be traceable to the subsystem from which they originated - The logging implementation should allow log entries to be annotated with source code location information like classname, methodname, file and line number, without requiring code changes - It should be possible to change the logging implementation used without having to change thousands of lines of code - The mapping of loggers to destinations (files, directories, servers etc.) should be specified and modifiable via configuration Uniform logging format: All Hadoop logs should have the following structure. \\n \\n [\\n] . . . where the header line specifies the format of each log entry. The header line has the format: '# ...\\n'. The default format of each log entry is: '# Timestamp Level LoggerName Message', where: - Timestamp is a date and time in the format MM/DD/YYYY:HH:MM:SS - Level is the logging level (FATAL, WARN, DEBUG, TRACE, etc.) - LoggerName is the short name of the logging subsystem from which the message originated e.g. fs.FSNam", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-219", "project": "HADOOP", "title": "SequenceFile#handleChecksumException NPE", "status": "Closed", "priority": "Trivial", "reporter": "Michael Stack", "created": "2006-05-16T01:20:43+00:00", "updated": "2006-08-03T17:46:42+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor. The latter is used by ReduceTask Sorter: java.lang.NullPointerException at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400) at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837) at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881) at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766) at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor. The latter is used by ReduceTask Sorter: java.lang.NullPointerException at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400) at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837) at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881) at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766) at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)\n\nComments:", "output": "SequenceFile#handleChecksumException NPE - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: SequenceFile#handleChecksumException NPE\n\nDescription: The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor. The latter is used by ReduceTask Sorter: java.lang.NullPointerException at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400) at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837) at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881) at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766) at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)", "output": "Priority: Trivial | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'SequenceFile#handleChecksumException NPE' and how was it resolved?", "context": "Title: SequenceFile#handleChecksumException NPE\n\nDescription: The SequenceFile#handleChecksumException assumes the conf data member has been set. It will not be set if we use the 'Reader(FileSystem fs, Path file, int bufferSize, long start, long length)' constructor. The latter is used by ReduceTask Sorter: java.lang.NullPointerException at org.apache.hadoop.io.SequenceFile$Reader.handleChecksumException(SequenceFile.java:407) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:400) at org.apache.hadoop.io.SequenceFile$Sorter$MergeStream.next(SequenceFile.java:837) at org.apache.hadoop.io.SequenceFile$Sorter$MergeQueue.merge(SequenceFile.java:881) at org.apache.hadoop.io.SequenceFile$Sorter$MergePass.run(SequenceFile.java:766) at org.apache.hadoop.io.SequenceFile$Sorter.mergePass(SequenceFile.java:702) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:528) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:253) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:787)", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-22", "project": "HADOOP", "title": "remove unused imports", "status": "Closed", "priority": "Trivial", "reporter": "Sami Siren", "created": "2006-02-07T04:33:17+00:00", "updated": "2006-08-03T17:46:26+00:00", "resolution": "Fixed"}, "description": "Following patch will remove unused imports from java source files", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Following patch will remove unused imports from java source files\n\nComments:", "output": "remove unused imports - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: remove unused imports\n\nDescription: Following patch will remove unused imports from java source files", "output": "Priority: Trivial | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'remove unused imports' and how was it resolved?", "context": "Title: remove unused imports\n\nDescription: Following patch will remove unused imports from java source files", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-220", "project": "HADOOP", "title": "Add -dfs and -jt command-line parameters to specify namenode and jobtracker.", "status": "Closed", "priority": "Major", "reporter": "Milind Barve", "created": "2006-05-16T05:18:08+00:00", "updated": "2006-08-03T17:46:42+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.\n\nComments:", "output": "Add -dfs and -jt command-line parameters to specify namenode and jobtracker. - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Add -dfs and -jt command-line parameters to specify namenode and jobtracker.\n\nDescription: Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Add -dfs and -jt command-line parameters to specify namenode and jobtracker.' and how was it resolved?", "context": "Title: Add -dfs and -jt command-line parameters to specify namenode and jobtracker.\n\nDescription: Most hadoop commands accept -df and -jt commandline parameters. Make the cp command to accept those as well.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-228", "project": "HADOOP", "title": "hadoop cp should have a -config option", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "created": "2006-05-18T07:12:00+00:00", "updated": "2006-08-03T17:46:42+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "hadoop cp should have a -config option to enable overriding of default parameters. it would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "hadoop cp should have a -config option to enable overriding of default parameters. it would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job\n\nComments:", "output": "hadoop cp should have a -config option - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: hadoop cp should have a -config option\n\nDescription: hadoop cp should have a -config option to enable overriding of default parameters. it would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'hadoop cp should have a -config option' and how was it resolved?", "context": "Title: hadoop cp should have a -config option\n\nDescription: hadoop cp should have a -config option to enable overriding of default parameters. it would perhaps be good to rename the command as well to something like dcp or distcp, since it's not a simple command, but rather an entire map-recude job", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-229", "project": "HADOOP", "title": "hadoop cp should generate a better number of map tasks", "status": "Closed", "priority": "Minor", "reporter": "Yoram Arnon", "created": "2006-05-18T07:22:37+00:00", "updated": "2006-08-03T17:46:42+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times. better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times. better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.\n\nComments:", "output": "hadoop cp should generate a better number of map tasks - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: hadoop cp should generate a better number of map tasks\n\nDescription: hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times. better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'hadoop cp should generate a better number of map tasks' and how was it resolved?", "context": "Title: hadoop cp should generate a better number of map tasks\n\nDescription: hadoop cp currently assigns 10 files to copy per map task. in case of a small number of large files on a large cluster (say 300 files of 30GB each on a 300 node cluster), this results in long execution times. better would be to assign files per task such that the entire cluster is utilized: one file per map, with a cap of 10000 maps total, so as not to over burden the job tracker.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-235", "project": "HADOOP", "title": "LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException", "status": "Closed", "priority": "Major", "reporter": "Benjamin Reed", "created": "2006-05-20T01:46:44+00:00", "updated": "2006-08-03T17:46:43+00:00", "resolution": "Fixed"}, "description": "openRaw should throw f.toString() on an error, not toString().", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "openRaw should throw f.toString() on an error, not toString().\n\nComments:", "output": "LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException\n\nDescription: openRaw should throw f.toString() on an error, not toString().", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException' and how was it resolved?", "context": "Title: LocalFileSystem.openRaw() throws the wrong string for FileNotFoundException\n\nDescription: openRaw should throw f.toString() on an error, not toString().", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-241", "project": "HADOOP", "title": "TestCopyFiles fails under cygwin due to incorrect path", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "created": "2006-05-24T01:22:23+00:00", "updated": "2006-08-03T17:46:43+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path. File.getPath() should be used to produce a consistent path.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path. File.getPath() should be used to produce a consistent path.\n\nComments:", "output": "TestCopyFiles fails under cygwin due to incorrect path - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: TestCopyFiles fails under cygwin due to incorrect path\n\nDescription: Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path. File.getPath() should be used to produce a consistent path.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'TestCopyFiles fails under cygwin due to incorrect path' and how was it resolved?", "context": "Title: TestCopyFiles fails under cygwin due to incorrect path\n\nDescription: Under cygwin TestCopyFiles generates an incorrect url which includes windows style path. This is the result of concatenation of a win path with unix path. File.getPath() should be used to produce a consistent path.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-245", "project": "HADOOP", "title": "record io translator doesn't strip path names", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-05-24T03:41:58+00:00", "updated": "2006-08-03T17:46:43+00:00", "assignee": "Milind Barve", "resolution": "Fixed"}, "description": "When I run the record translator with a pathname, the path name is not stripped. So for example: % bin/rcc --language c++ foo/bar/bat.jr generates: foo/bar/bat.jr.hh (instead of ./bat.jr.hh) and the first line is #ifndef __FOO/BAR/BAT_JR_HH__ the first was unexpected and the second is clearly wrong.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "When I run the record translator with a pathname, the path name is not stripped. So for example: % bin/rcc --language c++ foo/bar/bat.jr generates: foo/bar/bat.jr.hh (instead of ./bat.jr.hh) and the first line is #ifndef __FOO/BAR/BAT_JR_HH__ the first was unexpected and the second is clearly wrong.\n\nComments:", "output": "record io translator doesn't strip path names - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: record io translator doesn't strip path names\n\nDescription: When I run the record translator with a pathname, the path name is not stripped. So for example: % bin/rcc --language c++ foo/bar/bat.jr generates: foo/bar/bat.jr.hh (instead of ./bat.jr.hh) and the first line is #ifndef __FOO/BAR/BAT_JR_HH__ the first was unexpected and the second is clearly wrong.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'record io translator doesn't strip path names' and how was it resolved?", "context": "Title: record io translator doesn't strip path names\n\nDescription: When I run the record translator with a pathname, the path name is not stripped. So for example: % bin/rcc --language c++ foo/bar/bat.jr generates: foo/bar/bat.jr.hh (instead of ./bat.jr.hh) and the first line is #ifndef __FOO/BAR/BAT_JR_HH__ the first was unexpected and the second is clearly wrong.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-246", "project": "HADOOP", "title": "the record-io generated c++ has wrong comments", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-05-24T03:45:07+00:00", "updated": "2006-08-03T17:46:43+00:00", "assignee": "Milind Barve", "resolution": "Duplicate"}, "description": "The comments on the namespaces on the closing come out backward: } // end namespace org } // end namespace apache } // end namespace hadoop } // end namespace record } // end namespace test", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The comments on the namespaces on the closing come out backward: } // end namespace org } // end namespace apache } // end namespace hadoop } // end namespace record } // end namespace test\n\nComments:", "output": "the record-io generated c++ has wrong comments - Resolved: Duplicate"}, "classification": {"task": "classification", "input": "Title: the record-io generated c++ has wrong comments\n\nDescription: The comments on the namespaces on the closing come out backward: } // end namespace org } // end namespace apache } // end namespace hadoop } // end namespace record } // end namespace test", "output": "Priority: Major | Status: Closed | Resolution: Duplicate"}, "qa": {"task": "qa", "question": "What is the issue with 'the record-io generated c++ has wrong comments' and how was it resolved?", "context": "Title: the record-io generated c++ has wrong comments\n\nDescription: The comments on the namespaces on the closing come out backward: } // end namespace org } // end namespace apache } // end namespace hadoop } // end namespace record } // end namespace test", "answer": "The issue was resolved as: Duplicate"}}}
{"metadata": {"issue_key": "HADOOP-269", "project": "HADOOP", "title": "add FAQ to Wiki", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "created": "2006-06-02T02:04:14+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives.\n\nComments:", "output": "add FAQ to Wiki - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: add FAQ to Wiki\n\nDescription: Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'add FAQ to Wiki' and how was it resolved?", "context": "Title: add FAQ to Wiki\n\nDescription: Hadoop should have an FAQ in the Wiki. We can bootstrap this by reviewing the mailing list archives.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-272", "project": "HADOOP", "title": "bin/hadoop dfs -rm <dir> crashes in log4j code", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-06-03T06:03:04+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages: log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory) at java.io.FileOutputStream.openAppend(Native Method) at java.io.FileOutputStream.(FileOutputStream.java:177) at java.io.FileOutputStream.(FileOutputStream.java:102) at org.apache.log4j.FileAppender.setFile(FileAppender.java:289) at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163) at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215) at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96) at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654) at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612) at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468) at org.apache.log4j.LogManager.(LogManager.java:122) at org.apache.log4j.Logger.getLogger(Logger.java:104) at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229) at org.apache.commons.logging.impl.Log4JLogger.(Log4JLogger.java:65) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:494) at org.apache.commons.logging.impl.LogFactoryImpl.newInstance(LogFactoryImpl.java:529) at org.apache.commons.logging.impl.LogFactoryImpl.getInstance(LogFactoryImpl.java:235) at org.apache.commons.logging.LogFactory.getLog(LogFactory.java:370) at org.apache.hadoop.conf.Configuration.(Configuration.java:54) at org.apache.hadoop.dfs.DFSShell.main(DFSShell.java:307) log4j:ERROR Either File or DatePattern options are not set for appender [DRFA]. Delete failed", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages: log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory) at java.io.FileOutputStream.openAppend(Native Method) at java.io.FileOutputStream.(FileOutputStream.java:177) at java.io.FileOutputStream.(FileOutputStream.java:102) at org.apache.log4j.FileAppender.setFile(FileAppender.java:289) at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163) at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215) at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96) at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654) at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612) at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468) at org.apache.log4j.LogManager.(LogManager.java:122) at org.apache.log4j.Logger.getLogger(Logger.java:104) at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229) at org.apache.commons.logging.impl.Log4JLogger.(Log4JLogger.java:65) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constructor.java:494) at org.apache.commons.logging.impl.LogFactoryImpl.ne", "output": "bin/hadoop dfs -rm <dir> crashes in log4j code - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: bin/hadoop dfs -rm <dir> crashes in log4j code\n\nDescription: When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages: log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory) at java.io.FileOutputStream.openAppend(Native Method) at java.io.FileOutputStream.(FileOutputStream.java:177) at java.io.FileOutputStream.(FileOutputStream.java:102) at org.apache.log4j.FileAppender.setFile(FileAppender.java:289) at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163) at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215) at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96) at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654) at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612) at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468) at org.apache.log4j.LogManager.(LogManager.java:122) at org.apache.log4j.Logger.getLogger(Logger.java:104) at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229) at org.apache.commons.logging.impl.Log4JLogger.(Log4JLogger.java:65) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constr", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'bin/hadoop dfs -rm <dir> crashes in log4j code' and how was it resolved?", "context": "Title: bin/hadoop dfs -rm <dir> crashes in log4j code\n\nDescription: When I run \"bin/hadoop dfs -rm out-dir\" I get the following error messages: log4j:ERROR setFile(null,true) call failed. java.io.FileNotFoundException: /local/owen/hadoop/run/log (Is a directory) at java.io.FileOutputStream.openAppend(Native Method) at java.io.FileOutputStream.(FileOutputStream.java:177) at java.io.FileOutputStream.(FileOutputStream.java:102) at org.apache.log4j.FileAppender.setFile(FileAppender.java:289) at org.apache.log4j.FileAppender.activateOptions(FileAppender.java:163) at org.apache.log4j.DailyRollingFileAppender.activateOptions(DailyRollingFileAppender.java:215) at org.apache.log4j.config.PropertySetter.activate(PropertySetter.java:256) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:132) at org.apache.log4j.config.PropertySetter.setProperties(PropertySetter.java:96) at org.apache.log4j.PropertyConfigurator.parseAppender(PropertyConfigurator.java:654) at org.apache.log4j.PropertyConfigurator.parseCategory(PropertyConfigurator.java:612) at org.apache.log4j.PropertyConfigurator.configureRootCategory(PropertyConfigurator.java:509) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:415) at org.apache.log4j.PropertyConfigurator.doConfigure(PropertyConfigurator.java:441) at org.apache.log4j.helpers.OptionConverter.selectAndConfigure(OptionConverter.java:468) at org.apache.log4j.LogManager.(LogManager.java:122) at org.apache.log4j.Logger.getLogger(Logger.java:104) at org.apache.commons.logging.impl.Log4JLogger.getLogger(Log4JLogger.java:229) at org.apache.commons.logging.impl.Log4JLogger.(Log4JLogger.java:65) at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method) at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:39) at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:27) at java.lang.reflect.Constructor.newInstance(Constr", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-274", "project": "HADOOP", "title": "The new logging framework puts application logs into server directory in hadoop.log", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-06-05T23:26:26+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications). Thoughts?", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications). Thoughts?\n\nComments:", "output": "The new logging framework puts application logs into server directory in hadoop.log - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: The new logging framework puts application logs into server directory in hadoop.log\n\nDescription: The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications). Thoughts?", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'The new logging framework puts application logs into server directory in hadoop.log' and how was it resolved?", "context": "Title: The new logging framework puts application logs into server directory in hadoop.log\n\nDescription: The new logging infrastructure puts application logs into the server log directory under hadoop.log. I think it would be less confusing to use the old behavior of writing out to stderr (for applications). Thoughts?", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-276", "project": "HADOOP", "title": "No appenders could be found for logger", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-06-06T04:56:07+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "When you start the servers with an old configuration directory without the properties files, you get messages about: log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration). log4j:WARN Please initialize the log4j system properly. The problem is that the property files are not included in the jar file.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "When you start the servers with an old configuration directory without the properties files, you get messages about: log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration). log4j:WARN Please initialize the log4j system properly. The problem is that the property files are not included in the jar file.\n\nComments:", "output": "No appenders could be found for logger - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: No appenders could be found for logger\n\nDescription: When you start the servers with an old configuration directory without the properties files, you get messages about: log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration). log4j:WARN Please initialize the log4j system properly. The problem is that the property files are not included in the jar file.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'No appenders could be found for logger' and how was it resolved?", "context": "Title: No appenders could be found for logger\n\nDescription: When you start the servers with an old configuration directory without the properties files, you get messages about: log4j:WARN No appenders could be found for logger (org.apache.hadoop.conf.Configuration). log4j:WARN Please initialize the log4j system properly. The problem is that the property files are not included in the jar file.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-277", "project": "HADOOP", "title": "Race condition in Configuration.getLocalPath()", "status": "Closed", "priority": "Major", "reporter": "Peter Sutter", "created": "2006-06-06T08:11:33+00:00", "updated": "2006-08-03T17:46:45+00:00", "assignee": "Sameer Paranjpye", "resolution": "Fixed"}, "description": "(attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/map_15.out ... 060605 142531 task_0001_r_000009_1 0.31808624% reduce > append > /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142725 task_0001_r_000009_1 java.io.FileNotFoundException: /data2/tmp/mapred/local/task_0001_r_000009_1/map_15.out", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "(attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/map_15.out ... 060605 142531 task_0001_r_000009_1 0.31808624% reduce", "output": "Race condition in Configuration.getLocalPath() - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Race condition in Configuration.getLocalPath()\n\nDescription: (attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Race condition in Configuration.getLocalPath()' and how was it resolved?", "context": "Title: Race condition in Configuration.getLocalPath()\n\nDescription: (attached: a patch to fix the problem, and a logfile showing the problem occuring twice) There is a race condition in Configuration.java: Path file = new Path(dirs[index], path); Path dir = file.getParent(); if (fs.exists(dir) || fs.mkdirs(dir)) { return file; If two threads simultaneously process this code with the same target directory, fs.exists() will return false, but from fs.mkdirs() only one of the two threads will return true. From the Java documentation: \"returns: true if and only if the directory was created, along with all necessary parent directories; false otherwise\" That is, if the first thread successfully creates the directory, the second will not, and therefore return false, even though the directory exists. This was really happening. We use four temporary directories, and we had reducers failing all over the place with bizarre impossible errors. I modified the ReduceTaskRunner to output the filename that it creates to find the problem, and the log output is below. Here you can see copies initiated for two files that hash to the same temp directory, simultaneously. map_4.out is created in the correct directory (/data2...), but map_15.out is created in the next directory (/data3...) becuase of this race condition. Minutes later, when the appender tries to locate the file, that race condition does not occur (the directory already exists), and the appender looks for the file map_15.out in the correct directory, where it does not exist. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000004_0 output from rmr05. 060605 142414 task_0001_r_000009_1 Copying task_0001_m_000015_0 output from rmr04. ... 060605 142416 task_0001_r_000009_1 done copying task_0001_m_000004_0 output from rmr05 into /data2/tmp/mapred/local/task_0001_r_000009_1/map_4.out ... 060605 142418 task_0001_r_000009_1 done copying task_0001_m_000015_0 output from rmr04 into /data3/tmp/mapred/local/task_0001_r_000009_1/", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-30", "project": "HADOOP", "title": "DFS shell: support for ls -r and cat", "status": "Closed", "priority": "Major", "reporter": "Michel Tourn", "created": "2006-02-10T08:14:53+00:00", "updated": "2006-08-03T17:46:27+00:00", "resolution": "Fixed"}, "description": "patch attached", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "patch attached\n\nComments:", "output": "DFS shell: support for ls -r and cat - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: DFS shell: support for ls -r and cat\n\nDescription: patch attached", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'DFS shell: support for ls -r and cat' and how was it resolved?", "context": "Title: DFS shell: support for ls -r and cat\n\nDescription: patch attached", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-34", "project": "HADOOP", "title": "Build Paths Relative to PWD in build.xml", "status": "Closed", "priority": "Trivial", "reporter": "Jeremy Bensley", "created": "2006-02-14T02:00:26+00:00", "updated": "2006-08-03T17:46:27+00:00", "resolution": "Fixed"}, "description": "In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear: BUILD FAILED /home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory I have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile. I am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear: BUILD FAILED /home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory I have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile. I am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.\n\nComments:", "output": "Build Paths Relative to PWD in build.xml - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Build Paths Relative to PWD in build.xml\n\nDescription: In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear: BUILD FAILED /home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory I have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile. I am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.", "output": "Priority: Trivial | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Build Paths Relative to PWD in build.xml' and how was it resolved?", "context": "Title: Build Paths Relative to PWD in build.xml\n\nDescription: In the build.xml file, many paths are defined in terms of the present working directory (PWD) instead of relative to the location of the build.xml. Thus, whenever trying to compile from a directory other than the hadoop root, errors such as this appear: BUILD FAILED /home/jeremy/cvs/hadoop/build.xml:109: org.apache.jasper.JasperException: The -uriroot option must specify a pre-existing directory I have scripts / vim parameters that connect to other machines for compiling using ssh, and am not necessarily always in the root whenever I compile. I am attaching a patch which sets all paths relative to ${basedir}, and removes the override of ${basedir} to the PWD. Please let me know if there are reasons why the build environment must be relative to the working directory.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-36", "project": "HADOOP", "title": "Adding some uniformity/convenience to environment management", "status": "Closed", "priority": "Major", "reporter": "Bryan Pendleton", "created": "2006-02-14T05:03:38+00:00", "updated": "2006-08-03T17:46:27+00:00", "resolution": "Fixed"}, "description": "Currently, \"slaves\" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves Perhaps split slaves, having a different set for \"datanodes\" vs. \"tasktracker\" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption. Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable. These changes would probably be useful to the nutch project, too.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Currently, \"slaves\" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves Perhaps split slaves, having a different set for \"datanodes\" vs. \"tasktracker\" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption. Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable. These changes would probably be useful to the nutch project, too.\n\nComments:", "output": "Adding some uniformity/convenience to environment management - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Adding some uniformity/convenience to environment management\n\nDescription: Currently, \"slaves\" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves Perhaps split slaves, having a different set for \"datanodes\" vs. \"tasktracker\" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption. Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable. These changes would probably be useful to the nutch project, too.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Adding some uniformity/convenience to environment management' and how was it resolved?", "context": "Title: Adding some uniformity/convenience to environment management\n\nDescription: Currently, \"slaves\" are loaded from ~/.slaves. What would be better would be to default from something like conf/hadoop-slaves Perhaps split slaves, having a different set for \"datanodes\" vs. \"tasktracker\" nodes. ie, conf/hadoop-slaves-tasktracker, conf/hadoop-slaves-datanodes, or some similar split. There's the possibility it's worth building in the assumption that tasktracker is a superset, and thus implicitly includes datanodes, but this might be a bad assumption. Also, make sure all scripts source something like conf/hadoop-env.sh. Thus, the user can edit hadoop-env.sh to specify JAVA_HOME, or an alternate HADOOP_SLAVES location. It would also be desirable to have a seed CLASSPATH here. Possibly name it HADOOP_CLASSPATH, to make it explicit and not make hadoop scripts possibly interact with an otherwise-set system CLASSPATH variable. These changes would probably be useful to the nutch project, too.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-40", "project": "HADOOP", "title": "bufferSize argument is ignored in FileSystem.create(File, boolean, int)", "status": "Closed", "priority": "Minor", "reporter": "Konstantin Shvachko", "created": "2006-02-18T07:43:11+00:00", "updated": "2006-08-03T17:46:28+00:00", "resolution": "Fixed"}, "description": "org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize) ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config. MapReduce although is probably affected by that, see org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done) The attached patch would fix it.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize) ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config. MapReduce although is probably affected by that, see org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done) The attached patch would fix it.\n\nComments:", "output": "bufferSize argument is ignored in FileSystem.create(File, boolean, int) - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: bufferSize argument is ignored in FileSystem.create(File, boolean, int)\n\nDescription: org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize) ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config. MapReduce although is probably affected by that, see org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done) The attached patch would fix it.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'bufferSize argument is ignored in FileSystem.create(File, boolean, int)' and how was it resolved?", "context": "Title: bufferSize argument is ignored in FileSystem.create(File, boolean, int)\n\nDescription: org.apache.hadoop.fs.FileSystem.create(File f, boolean overwrite, int bufferSize) ignores the input parameter bufferSize. It passes further down the internal configuration, which includes the buffer size, but not the parameter value. This works fine within the file system, since everything that calls create extracts buffer size from the same config. MapReduce although is probably affected by that, see org.apache.hadoop.io.SequenceFile.Sorter.MergeQueue.MergeQueue(int size, String outName, boolean done) The attached patch would fix it.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-41", "project": "HADOOP", "title": "JAVA_OPTS for the TaskRunner Child", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-02-18T09:47:33+00:00", "updated": "2006-08-03T17:46:28+00:00", "resolution": "Fixed"}, "description": "Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes. Examples of why its useful being able to set options are the child include: + Being able to set '-server' option or '-c64'. + Passing logging.properties to configure child logging. + Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process. Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes. Examples of why its useful being able to set options are the child include: + Being able to set '-server' option or '-c64'. + Passing logging.properties to configure child logging. + Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process. Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.\n\nComments:", "output": "JAVA_OPTS for the TaskRunner Child - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: JAVA_OPTS for the TaskRunner Child\n\nDescription: Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes. Examples of why its useful being able to set options are the child include: + Being able to set '-server' option or '-c64'. + Passing logging.properties to configure child logging. + Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process. Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'JAVA_OPTS for the TaskRunner Child' and how was it resolved?", "context": "Title: JAVA_OPTS for the TaskRunner Child\n\nDescription: Currently, its possible to set the java heap size the TaskRunner child runs in, but thats all thats configurable about the child process. Hereabouts, we've found it useful being able to specify other options for the child JVM, especially when debugging and monitoring long-lived processes. Examples of why its useful being able to set options are the child include: + Being able to set '-server' option or '-c64'. + Passing logging.properties to configure child logging. + Enable and capture to file verbose GC logging or start the SUN JVM JMX agent for the child process. Allows connecting with jconsole to watch long-lived children, their heap and thread usage, and when seemingly hung, take thread dumps.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-42", "project": "HADOOP", "title": "PositionCache decrements its position for reads at the end of file", "status": "Closed", "priority": "Major", "reporter": "Konstantin Shvachko", "created": "2006-02-18T09:58:44+00:00", "updated": "2006-08-03T17:46:28+00:00", "resolution": "Fixed"}, "description": "See int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained. The attached patch would fix it.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "See int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained. The attached patch would fix it.\n\nComments:", "output": "PositionCache decrements its position for reads at the end of file - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: PositionCache decrements its position for reads at the end of file\n\nDescription: See int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained. The attached patch would fix it.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'PositionCache decrements its position for reads at the end of file' and how was it resolved?", "context": "Title: PositionCache decrements its position for reads at the end of file\n\nDescription: See int org.apache.hadoop.fs.FSDataInputStream.PositionCache.read(byte[] b, int off, int len) if in.read() returns -1 (e.g. at the end of file) the position in the cache will be decremented, while it should be retained. The attached patch would fix it.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-44", "project": "HADOOP", "title": "RPC exceptions should include remote stack trace", "status": "Closed", "priority": "Major", "reporter": "Doug Cutting", "created": "2006-02-22T07:06:46+00:00", "updated": "2006-08-03T17:46:28+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging.\n\nComments:", "output": "RPC exceptions should include remote stack trace - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: RPC exceptions should include remote stack trace\n\nDescription: Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'RPC exceptions should include remote stack trace' and how was it resolved?", "context": "Title: RPC exceptions should include remote stack trace\n\nDescription: Remote exceptions currently only report the exception string. Instead they should report the entire remote stack trace, as a string, to facilitate debugging.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-5", "project": "HADOOP", "title": "need commons-logging-api jar file", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "created": "2006-02-07T03:04:15+00:00", "updated": "2006-08-03T17:46:26+00:00", "resolution": "Fixed"}, "description": "The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.\n\nComments:", "output": "need commons-logging-api jar file - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: need commons-logging-api jar file\n\nDescription: The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'need commons-logging-api jar file' and how was it resolved?", "context": "Title: need commons-logging-api jar file\n\nDescription: The hadoop lib directory needs a copy of the commons-logging-api jar file from nutch's lib directory.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-58", "project": "HADOOP", "title": "Hadoop requires configuration of hadoop-site.xml or won't run", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-02-24T03:43:08+00:00", "updated": "2006-08-03T17:46:29+00:00", "resolution": "Won't Fix"}, "description": "On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost. Currently this is not the case. Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'. It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties. Revision: 379930", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost. Currently this is not the case. Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'. It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties. Revision: 379930\n\nComments:", "output": "Hadoop requires configuration of hadoop-site.xml or won't run - Resolved: Won't Fix"}, "classification": {"task": "classification", "input": "Title: Hadoop requires configuration of hadoop-site.xml or won't run\n\nDescription: On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost. Currently this is not the case. Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'. It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties. Revision: 379930", "output": "Priority: Minor | Status: Closed | Resolution: Won't Fix"}, "qa": {"task": "qa", "question": "What is the issue with 'Hadoop requires configuration of hadoop-site.xml or won't run' and how was it resolved?", "context": "Title: Hadoop requires configuration of hadoop-site.xml or won't run\n\nDescription: On a new install, I would expect '${HADOOP_HOME}/bin/start-all.sh\" to bring up a basic instance, one that is using local filesystem (Or if not, then uses a DFS homed in localhost:/tmp) and that has all four daemons running on localhost. Currently this is not the case. Hadoop complains 'java.lang.RuntimeException: Not a host:port pair: local'. It doesn't like the 'local' default value for mapred.job.tracker and fs.default.name properties. Revision: 379930", "answer": "The issue was resolved as: Won't Fix"}}}
{"metadata": {"issue_key": "HADOOP-6", "project": "HADOOP", "title": "missing build directory in classpath", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "created": "2006-02-07T03:19:03+00:00", "updated": "2006-08-03T17:46:26+00:00", "resolution": "Fixed"}, "description": "When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.\n\nComments:", "output": "missing build directory in classpath - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: missing build directory in classpath\n\nDescription: When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'missing build directory in classpath' and how was it resolved?", "context": "Title: missing build directory in classpath\n\nDescription: When running a developer build, the hadoop script needs the build directory on the classpath so that the job tracker can find the webapps directory.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-60", "project": "HADOOP", "title": "Specification of alternate conf. directory", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-02-28T01:56:17+00:00", "updated": "2006-08-03T17:46:29+00:00", "resolution": "Fixed"}, "description": "Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct. Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct. Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.\n\nComments:", "output": "Specification of alternate conf. directory - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: Specification of alternate conf. directory\n\nDescription: Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct. Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'Specification of alternate conf. directory' and how was it resolved?", "context": "Title: Specification of alternate conf. directory\n\nDescription: Currently, hadoop configuration must be done by making edits and addition to ${HADOOP_HOME}/conf. Allowing specification of an alternate configuration directory will allow keeping configuration and binary distinct. Benefits include: Binary can be made read-only; or binary is blanket-updateable with configuration undisturbed.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-62", "project": "HADOOP", "title": "can't get environment variables from HADOOP_CONF_DIR", "status": "Closed", "priority": "Minor", "reporter": "Owen O'Malley", "created": "2006-03-02T08:49:53+00:00", "updated": "2006-08-03T17:46:29+00:00", "assignee": "Owen O'Malley", "resolution": "Duplicate"}, "description": "The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.\n\nComments:", "output": "can't get environment variables from HADOOP_CONF_DIR - Resolved: Duplicate"}, "classification": {"task": "classification", "input": "Title: can't get environment variables from HADOOP_CONF_DIR\n\nDescription: The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.", "output": "Priority: Minor | Status: Closed | Resolution: Duplicate"}, "qa": {"task": "qa", "question": "What is the issue with 'can't get environment variables from HADOOP_CONF_DIR' and how was it resolved?", "context": "Title: can't get environment variables from HADOOP_CONF_DIR\n\nDescription: The bin/hadoop script doesn't use the HADOOP_CONF_DIR variable to find hadoop-env.sh.", "answer": "The issue was resolved as: Duplicate"}}}
{"metadata": {"issue_key": "HADOOP-65", "project": "HADOOP", "title": "add a record I/O framework to hadoop", "status": "Closed", "priority": "Minor", "reporter": "Sameer Paranjpye", "created": "2006-03-07T07:12:40+00:00", "updated": "2006-08-03T17:46:29+00:00", "resolution": "Fixed"}, "description": "Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.\n\nComments:", "output": "add a record I/O framework to hadoop - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: add a record I/O framework to hadoop\n\nDescription: Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'add a record I/O framework to hadoop' and how was it resolved?", "context": "Title: add a record I/O framework to hadoop\n\nDescription: Hadoop could benefit greatly from a simple record I/O framework that enables the specification of simple record types and enables the generation of code for serialization/deserialization in multiple target languages. The framework would handle a small well understood set of primitive types and simple compositions of these (structs, vectors, maps) . It would be possible to leverage this framework to express I/O in MapReduce computations and to use this as the basis for Hadoops RPC implementation. This would make interfacing with code in languages other than Java much easier.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-78", "project": "HADOOP", "title": "rpc commands not buffered", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-03-14T06:48:27+00:00", "updated": "2006-08-03T17:46:31+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "Calls using Hadoop's RPC framework get sent across the network byte by byte.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "Calls using Hadoop's RPC framework get sent across the network byte by byte.\n\nComments:", "output": "rpc commands not buffered - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: rpc commands not buffered\n\nDescription: Calls using Hadoop's RPC framework get sent across the network byte by byte.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'rpc commands not buffered' and how was it resolved?", "context": "Title: rpc commands not buffered\n\nDescription: Calls using Hadoop's RPC framework get sent across the network byte by byte.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-80", "project": "HADOOP", "title": "binary key", "status": "Closed", "priority": "Major", "reporter": "Owen O'Malley", "created": "2006-03-14T15:18:51+00:00", "updated": "2006-08-03T17:46:31+00:00", "assignee": "Owen O'Malley", "resolution": "Fixed"}, "description": "I needed a binary key type, so I extended BytesWritable to be comparable also.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "I needed a binary key type, so I extended BytesWritable to be comparable also.\n\nComments:", "output": "binary key - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: binary key\n\nDescription: I needed a binary key type, so I extended BytesWritable to be comparable also.", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'binary key' and how was it resolved?", "context": "Title: binary key\n\nDescription: I needed a binary key type, so I extended BytesWritable to be comparable also.", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-82", "project": "HADOOP", "title": "JobTracker loses it: NoSuchElementException", "status": "Closed", "priority": "Minor", "reporter": "Michael Stack", "created": "2006-03-16T01:15:11+00:00", "updated": "2006-08-03T17:46:31+00:00", "resolution": "Fixed"}, "description": "On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log. 060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException java.io.IOException: java.util.NoSuchElementException at java.util.TreeMap.key(TreeMap.java:433) at java.util.TreeMap.firstKey(TreeMap.java:287) at java.util.TreeSet.first(TreeSet.java:407) at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read(BufferedInputStream.java:235) at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210) at java.io.DataInputStream.readInt(DataInputStream.java:353) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557) at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666) I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks. Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it: 060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org 060314 204758 Task 'task_m_4d6ht0' has been lost. 060314 204811 Adding task 'task_m_fb0wf0' to tip tip_fizr7m, for tracker 'tracker_70065' on ia109314.archive.org 060314 210118 Task 'task_m_fb0wf0' has been lost. 060314 210119 Adding task 'task_m_irar47' to tip tip_fizr7m, for tracker 'tracker_82285' on ia109324.archive.org 060314 211541 Taskid 'task_m_irar47' has finished successfully. 060314 211541 Task 'task_m_irar47' has completed. 060314 211543 Adding task 'task_m_qo1g69' to tip tip_fizr7m, for tracker 'tracker_97839' on ia109306.archive.org 060314 213004 Taskid 'task_m_qo1g69' has finished successfully. 060314 213004 Task 'task_m_qo1g69' has completed. 060314 213005 Adding task 'task_m_t0lnzk' to tip tip_fizr7m, for tracker 'tracker_57273' on ia109314.archive.org 060314 214118 Task 'task_m_t0lnzk' has been lost. So, we lose two, complete two, then lose a third. TIP should have been done on first completion. TIP accounting is off.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log. 060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException java.io.IOException: java.util.NoSuchElementException at java.util.TreeMap.key(TreeMap.java:433) at java.util.TreeMap.firstKey(TreeMap.java:287) at java.util.TreeSet.first(TreeSet.java:407) at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read(BufferedInputStream.java:235) at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210) at java.io.DataInputStream.readInt(DataInputStream.java:353) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557) at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666) I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks. Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it: 060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org 060314 204758 Task 'task_m_4d6ht0' has been lost. 060314 204811 Addi", "output": "JobTracker loses it: NoSuchElementException - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: JobTracker loses it: NoSuchElementException\n\nDescription: On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log. 060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException java.io.IOException: java.util.NoSuchElementException at java.util.TreeMap.key(TreeMap.java:433) at java.util.TreeMap.firstKey(TreeMap.java:287) at java.util.TreeSet.first(TreeSet.java:407) at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read(BufferedInputStream.java:235) at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210) at java.io.DataInputStream.readInt(DataInputStream.java:353) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557) at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666) I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks. Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it: 060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org 060", "output": "Priority: Minor | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'JobTracker loses it: NoSuchElementException' and how was it resolved?", "context": "Title: JobTracker loses it: NoSuchElementException\n\nDescription: On a number of occasions, JobTracker goes into a loop that it never recovers from. Over and over it prints the below to the jobtracker log. 060304 124522 Server handler 5 on 8010 call error: java.io.IOException: java.util.NoSuchElementException java.io.IOException: java.util.NoSuchElementException at java.util.TreeMap.key(TreeMap.java:433) at java.util.TreeMap.firstKey(TreeMap.java:287) at java.util.TreeSet.first(TreeSet.java:407) at org.apache.hadoop.mapred.TaskInProgress.getTaskToRun(TaskInProgress.java:428Timed out.org.apache.hadoop.fs.ChecksumException: Checksum error:/2/hadoop/nara/data/tmp/task_r_m80hob/all.1 at 1554810368 at org.apache.hadoop.fs.FSDataInputStream$Checker.verifySum(FSDataInputStream.java:122) at org.apache.hadoop.fs.FSDataInputStream$Checker.read(FSDataInputStream.java:98) at org.apache.hadoop.fs.FSDataInputStream$PositionCache.read(FSDataInputStream.java:158) at java.io.BufferedInputStream.fill(BufferedInputStream.java:218) at java.io.BufferedInputStream.read(BufferedInputStream.java:235) at org.apache.hadoop.fs.FSDataInputStream$Buffer.read(FSDataInputStream.java:210) at java.io.DataInputStream.readInt(DataInputStream.java:353) at org.apache.hadoop.io.SequenceFile$Reader.next(SequenceFile.java:367) at org.apache.hadoop.io.SequenceFile$Sorter$SortPass.run(SequenceFile.java:557) at org.apache.hadoop.io.SequenceFile$Sorter.sortPass(SequenceFile.java:523) at org.apache.hadoop.io.SequenceFile$Sorter.sort(SequenceFile.java:511) at org.apache.hadoop.mapred.ReduceTask.run(ReduceTask.java:254) at org.apache.hadoop.mapred.TaskTracker$Child.main(TaskTracker.java:666) I added debug to TIP#getTaskToRun so i could tell which TIP had empted its allotment of tasks. Below is extract from jobtracker log that shows sequence of events for TIP tip_fizr7m that lead up to JT losing it: 060314 203637 Adding task 'task_m_4d6ht0' to tip tip_fizr7m, for tracker 'tracker_41791' on ia109314.archive.org 060", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-86", "project": "HADOOP", "title": "If corrupted map outputs, reducers get stuck fetching forever", "status": "Closed", "priority": "Major", "reporter": "Michael Stack", "created": "2006-03-17T01:58:09+00:00", "updated": "2006-08-03T17:46:32+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException. Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside). And so it goes till the cows come home. Doug applied a patch that in map output file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again). This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts. A further patch that clears the severe flag was posted to the list. This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs. During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail). This issue covers implementing a better solution. Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT. A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates JT when corrupted map output.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException. Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside). And so it goes till the cows come home. Doug applied a patch that in map output file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again). This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts. A further patch that clears the severe flag was posted to the list. This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs. During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail). This issue covers implementing a better solution. Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT. A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates JT when corrupted map output.\n\nComments:", "output": "If corrupted map outputs, reducers get stuck fetching forever - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: If corrupted map outputs, reducers get stuck fetching forever\n\nDescription: In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException. Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside). And so it goes till the cows come home. Doug applied a patch that in map output file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again). This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts. A further patch that clears the severe flag was posted to the list. This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs. During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail). This issue covers implementing a better solution. Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT. A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates ", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'If corrupted map outputs, reducers get stuck fetching forever' and how was it resolved?", "context": "Title: If corrupted map outputs, reducers get stuck fetching forever\n\nDescription: In our rack, there is a machine that reliably corrupts map output parts. When reducers try to pickup the map output, Server#Handler checks the checksum, notices corruption, moves the bad map output part aside and throws a ChecksumException. Undeterred, the reducer comes back again minutes later only this time it gets a FileNotFoundException out of Server#Handler (Because the part was moved aside). And so it goes till the cows come home. Doug applied a patch that in map output file, when it notices a fatal exception, it logs a severe error on the TaskTracker#LOG. Then in TT, if a severe logging has occurred, TT does a soft restart (TT stays up but closes down all services and then goes through init again). This patch was committed (after I suggested it was working), only, later, I noticed the severe log flag is not cleared across TT restart so TT goes into a cycle of continuous restarts. A further patch that clears the severe flag was posted to the list. This improves things but has issues too in that on revival, the TT continues to be plagued by reducers looking for parts no longer available for a period of ten minutes or so until the JobTracker gets around to updating them about change in where to go get map outputs. During this period, the TT gets restarted 5-10 times -- but eventually comes back on line (There may have been too much damage done during this period of flux making it so the job will fail). This issue covers implementing a better solution. Suggestions include having the TT stay down a period to avoid the incoming reducers or somehow examining the incoming reducer request, checking its list of tasks to see if it knows anything of the reducers' request and rejecting it with a non-severe error if not a map of the currently running TT. A little birdie (named DC) suggests a better soln. is probably an addition to intertrackerprotocol so either the TT or the reducer updates ", "answer": "The issue was resolved as: Fixed"}}}
{"metadata": {"issue_key": "HADOOP-87", "project": "HADOOP", "title": "SequenceFile performance degrades substantially compression is on and large values are encountered", "status": "Closed", "priority": "Major", "reporter": "Sameer Paranjpye", "created": "2006-03-17T06:49:48+00:00", "updated": "2006-08-03T17:46:32+00:00", "assignee": "Doug Cutting", "resolution": "Fixed"}, "description": "The code snippet in quesiton is: if (deflateValues) { deflateIn.reset(); val.write(deflateIn); deflater.reset(); deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength()); deflater.finish(); while (!deflater.finished()) { int count = deflater.deflate(deflateOut); buffer.write(deflateOut, 0, count); } } else { A couple of issues with this code: 1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem. 2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'. Proposed fix: 1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size. 2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, needs to shrink as well.", "comments": [], "tasks": {"summarization": {"task": "summarization", "input": "The code snippet in quesiton is: if (deflateValues) { deflateIn.reset(); val.write(deflateIn); deflater.reset(); deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength()); deflater.finish(); while (!deflater.finished()) { int count = deflater.deflate(deflateOut); buffer.write(deflateOut, 0, count); } } else { A couple of issues with this code: 1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem. 2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'. Proposed fix: 1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size. 2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, needs to shrink as well.\n\nComments:", "output": "SequenceFile performance degrades substantially compression is on and large values are encountered - Resolved: Fixed"}, "classification": {"task": "classification", "input": "Title: SequenceFile performance degrades substantially compression is on and large values are encountered\n\nDescription: The code snippet in quesiton is: if (deflateValues) { deflateIn.reset(); val.write(deflateIn); deflater.reset(); deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength()); deflater.finish(); while (!deflater.finished()) { int count = deflater.deflate(deflateOut); buffer.write(deflateOut, 0, count); } } else { A couple of issues with this code: 1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem. 2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'. Proposed fix: 1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size. 2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, need", "output": "Priority: Major | Status: Closed | Resolution: Fixed"}, "qa": {"task": "qa", "question": "What is the issue with 'SequenceFile performance degrades substantially compression is on and large values are encountered' and how was it resolved?", "context": "Title: SequenceFile performance degrades substantially compression is on and large values are encountered\n\nDescription: The code snippet in quesiton is: if (deflateValues) { deflateIn.reset(); val.write(deflateIn); deflater.reset(); deflater.setInput(deflateIn.getData(), 0, deflateIn.getLength()); deflater.finish(); while (!deflater.finished()) { int count = deflater.deflate(deflateOut); buffer.write(deflateOut, 0, count); } } else { A couple of issues with this code: 1. The value is serialized to the 'deflateIn' buffer which is an instance of 'DataOutputBuffer', this grows as large as needed to store the serialized value and stays as large as the largest serialized value encountered. If, for instance a stream has a single 8MB value followed by several 8KB values the size of the buffer stays at 8MB. The problem is that the *entire* 8MB buffer is always copied over the JNI boundary regardless of the size of the value. We've observed this over several runs where compression performance degrades by a couple of orders of magnitude when a very large value is encountered. Shrinking the buffer fixes the problem. 2. Data is copied lots of times. First the value is serialized into 'deflateIn'. Second, the value is copied over the JNI boundary in *every* iteration of the while loop. Third, the compressed data is copied piecemeal into 'deflateOut'. Finally, it is appended to 'buffer'. Proposed fix: 1. Don't let big buffers persist. Allow 'deflateIn' to grow to a *persistent* maximum reasonable size, say 64KB. If a larger value is encountered, grow the buffer in order to process the value, then shrink it back to the maximum size. To do this, we add a 'reset' method which takes a buffer size. 2. Don't use a loop to deflate. The maximum size of the output can be determined by 'maxOutputSize = inputSize * 1.01 + 12'. This is the maximum output size that zlib will produce. We allocate a large enough output buffer and compress everything in 1 pass. The output buffer, of course, need", "answer": "The issue was resolved as: Fixed"}}}
